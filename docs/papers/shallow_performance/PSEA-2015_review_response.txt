

> 1. The results can be broadly separated into i) comparison with the
> original 'shallow' benchmark and ii) Optimisation of the Psy
> layer. They are presented in this order but to my mind this seems like
> the wrong way round. I don't think that there is technically a 'right
> way round' as they are very separate, but I did expect to read about
> the optimisation first. Also while reading the document, it mentioned
> 'original and the (best) PsyKAl ...' on page 6, it left me wondering
> what 'best' is because I hadn't reached the bit about optimisation at
> that stage. Suggest to reorder.

We have split the Results section into three subsections and
re-ordered, as suggested.

> Other

> 2. space after 'PsyKal' and before ',' on second line of abstract.
Fixed.

> 3. para 2, page 2: should there be 'of' on line 3? in the
  programming of models?

We're talking here about programming models such as OpenMP, CUDA etc. rather
than about constructing scientific models.

> 4. para 4, page 3: is there a version number of shallow used in the tests?

GRAHAM.

> 5. last full para, page 3: x- and y-directions?
Fixed.

> 6. para 1, page 4: is it one third per section?
Re-worded.

> 7. page 5, second transformation: remove second 'as'
Fixed.

> 8. para 3, page 6: 64, 128 ... should these be squared also on the figure too.
Fixed.

> 9. para 4, page 6: in the statement 'for the first two bars...' I
  would say that this is also true for the third bar also. This is gnu
  on the IvyBridge. Is that understood?
Text has been added to explain this.

> 11. para6, page 6: difference seems to be in italics.
Fixed.

> 12. para 2, page 8: could you reword this; "on Haswell loop fusion
 achieves more than 90% while on Ivy Bridge it is fully-fusing the
 first loop nest", I found it difficult to follow.
Re-worded.

> 13. Figure 4 and labeling the transformations: The fact that you
  have labeled the run instances 0-10 means its difficult to compare
  the labels with the bars (for me at least). Rather than reading the
  shading, I found my self counting them and I kept getting mixed up
  because the sixth bar corresponds to transformation 5. Further, in
  the text on page 9, there is reference to bar 6 which I assume is
  transformation 6 but bar 7... I understand why its been laid out
  this way but I think the transformations/vanilla could be labeled
  1-11?
We agree and have re-numbered the transformations from 1-11. We have
also added labels to the key optimisation steps in the Figure and
refer to the transformations by number in the text.

> 14. para 3, page 9: potentially change "operation of Figure 5 is
  fused" to "operation (lower loop in Figure 5) is fused."
Re-worded.

> 15. pare 1, page 10: in the sentence; "remain constant for the
> duration of each trip of the time-stepping loop" I've never heard
> that terminology before, is that standard? I would expect to hear
> something like "remain constant for the duration of each iteration
> of the time-stepping loop"
Re-worded.

> 16. the introduction of Figure 5 help to explain a bit more the
> detail of the applied transformations. This was good and it would
> have been nice to have had a bit more detail on the transformations
> from the start rather than disbursed through the text.

DOES SPACE PERMIT ANY MORE DETAIL?

----------------------- REVIEW 2 ---------------------

> The paper presents a very detailed and rigorous analysis of 6 such
> combinations across 3 platforms and 4 compilers with overall promising
> results. However the description of the PSyKAI architecture and the
> responsibilities and interplay between the different layers does not
> become entirely clear. The authors present very little background and
> rationale as to how PSyKAI compares to similar approaches in the
> field. It should also be made very clear in the introduction that the
> PSyKAI layer needs to presently be written *manually*, since this
> makes the current approach effectively "hand-optimised code".

We have altered the text to make it clearer that in this work we are
manually optimising the PSy layer in order to learn what
transformations must be supported in our code generation system (which
will be the subject of a future paper).

> Clarification questions
> -----------------------

> Introduction

> 1) p2: In which way has the PSyKAI design been influenced by OP2? How does it compare?

RUPERT.

> 2) p2: It is unclear what is meant by "local fields" in the kernel
> context: what is the granularity? Does a kernel (as written by the
> scientist) operate on the entire partition? A colum? A vertical
> slice of the partition? A single point? For the PSyKAI layer to be
> automatically generated, the kernel interface would need to be be
> well defined.

The text has been re-worked slightly to emphasise that a Kernel, like
the Algorithm layer, has no concept of parallelism. The nature of the
field it operates on depends on the application.

> 3) p2: If the algorithm layer operates on a full field and the kernels
> on "local fields", is the PSyKAI layer responsible for the domain
> decomposition?

Yes it is. The text has been re-worked to emphasise this.

> 4) p3: Who implemented the "original" Shallow version benchmarked
> against and for which platform has it been optimised?

Paul Swarztrauber originally wrote the code but the header text
records modifications by at least two other people. The hardware that
they were optimising for is not known. A line has been added to this effect.

> 5) p3: What does "retaining the I/O capability" imply? Is bitwise
> reproducability of results a requirement?

It simply implies that the solution fields can be written to disk at a
user-defined interval. Text has been added to this effect. Bit-wise
reproducibility is not a requirement.

> 6) p4: it is unclear from Figure 1 and the description on the previous
> page whether invoke_tstep is a routine written by the scientist and
> 'transformed' or entirely the responsibility of PSyKAI. It the latter
> is the case, how can PSyKAI determine which kernels to call, in which
> order, which parameters to pass etc.?

PSyKAl is a design approach, not a tool. Figure 1 illustrates the
structure of the Shallow code after it has been re-structured
following PSyKAl principles. How the code in the different layers
is produced is not the subject of this paper.

> Methodology

> 7) p5: Some of the transformations (3, 4, 5, 7, 9, 10) seem to be very
> specific to the Shallow application. In which way do those represent
> optimisations 'independent of the science code' as claimed?

Of necessity, the code transformations required to recover performance
for a specific code are determined by the structure of that code. We
are not claiming that a general set of optimisations can be applied to
any given code. Instead, the PSyKAl approach separates the code which
must be optimised from the code describing the science. The
optimisations are restricted to the PSy layer and therefore do not
affect the scientific components of the code.

> 8) p5: Could transformations be encoded as rules in a reusable form?
> Presently every version of the PSyKAI layer is presumably a separate,
> independent implementation?

In this paper we are concerned with whether (and how) it is possible
to recover performance after applying the considerable restructuring
required to achieve the PSyKAl separation of concerns. To do this we
have manually written the PSy layer. As the reviewer correctly states,
this means we would have a separate implementation for each target
architecture; precisely the situation we are working to avoid. We are
therefore developing the PSyclone code-generation system to generate
and manipulate the PSy layer. This will be the subject of a future
paper.

> Conclusions

> 9) p11: As pointed out in the comments above it seems the PSyKAI
> layer would require more tightly specified interface to both the
> algorithmic and kernel layers to be suitable for automatic
> generation. It would be interesting to hear the authors' thoughts on
> this.

The reviewer is correct, however automatic generation is not the
subject of this paper. Suffice to say we have such interfaces and a
python-based code-generation system (PSyclone) that can generate correct
code. This wil be the subject of a future paper.

> 10) p11: How would these user-provided transformation recipes look like?

These recipes are implemented as python scripts that apply PSyclone
Transformations.

General comments / suggestions
------------------------------

> 1) There seems to be no preprint available yet for reference [2]

This is correct. The paper referred to has been accepted for publication
but we are awaiting notification from the publisher as to when
it will actually appear.

> 2) Figure 2/3: A better visual grouping of the bars by platform and
> compiler e.g. by using colour to differentiate compilers and hatching
> to different platforms would help with interpretation of the plots
> 3) Figure 4: The bars per cluster are quite hard to tell apart visually

We would love to use colour but the formatting guidlines say we
can't. However, we have added additional labelling to what was Figure
3 in order to aid interpretation.




%% nemolite2d_perf.tex
%%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.

\usepackage{cite}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.eps}
\fi


% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
\usepackage{dblfloatfix}


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.

\usepackage{url}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newlength{\picwidth}
\setlength{\picwidth}{85mm}

\newcommand{\psykal}{{PS}y{KA}l\ }

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Something about Performance, 2D Component of NEMO. (TBC)}
%
% author names and IEEE memberships
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{J.~Appleyard, M.~Ashworth, R.~W.~Ford, J.~Holt, H.~Liu, A.~R.~Porter and G.~D.~Riley% <-this % stops a space
\thanks{J.~Appleyard is with NVIDIA.}%
\thanks{H.~Liu and J.~Holt are with the National Oceanography Centre, Liverpool, UK.}%
\thanks{M.~Ashworth, R.~W.~Ford and A.~R.~Porter are with the Science and Technology Facilities Council, UK.}% <-this % stops a space
\thanks{G.~Riley is with the University of Manchester, Manchester, UK.}% <-this % stops a space
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names.


% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~11, No.~4, December~2012}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2012 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
The abstract goes here.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Performance, Code-generation, Finite-difference
\end{IEEEkeywords}



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HE" in caps to complete the first word.
\IEEEPARstart{T}{he} challenge presented to the developers of
scientific software by the drive towards Exascale computing is
considerable. With power consumption becoming the overriding design
constraint, CPU clock speeds are falling and the complex,
multi-purpose compute core is being replaced by multiple, simpler
cores. This philosophy can be seen at work in the rise of so-called
accelerator-based machines in the Top 500 List~\cite{top500} of
supercomputers: four of the top-ten machines in the November 2015 list
make use of Intel Xeon Phi's or NVIDIA GPUs. Two of the remaining six
machines are IBM BlueGene/Qs, the CPU of which has hardware support
for running 64 threads.

Achieving good performance on large numbers of light-weight cores
requires exploiting as much parallelism in an application as possible
and this results in increased complexity in the programming models
that must be used. This in turn increases the burden of code
maintenance and code development, in part because two specialisms are
required: that of the scientific domain which a code is modelling
({\it e.g.} oceanography) and that of computational science. The
situation is currently complicated still further by the existence of
competing hardware technology; if one was to begin writing a major
scientific application today it is unclear whether one would target
GPU, Xeon Phi, traditional CPU, FPGA or something else entirely. This
is a problem because, generally speaking, these different technologies
require different programming approaches.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\subsection{The \psykal Approach}
Proliferation of lightweight cores is resulting in increased
complexity in the programming models required to achieve performance.
Combine with the complexity of e.g. a model simulating circulation in
the global ocean. Significant challenge in software engineering.
Propose an approach for meeting this challenge by separating the
computational science demands from the {\it e.g.} oceanographic
demands.

\begin{figure}
\centering
\caption{The \psykal Approach.}
\end{figure}

While encapsulation makes it easier to produce more robust software,
it can often come with a performance penalty. In this work we
investigate in detail the hardware and compiler dependencies of this
penalty and the steps required to remove it.

\subsection{The `nemolite2d' Program}

For this work we have used a program "nemolite2d" developed by
ourselves.  “nemolite2d” is a vertically-averaged version of NEMO
(Nucleus for European Modelling of the Ocean), which only keeps its
dynamical part. The whole model system is represented through one
continuity equation (for the update of the sea-surface height) and two
vertically-integrated momentum equations (for the two velocity
components, respectively).

(add above three equations in .tex file).

The external forcing includes surface wind stress, bottom friction,
and open boundary barotropic forcing. A lateral slip boundary
condition is applied along the coast lines. The open boundary
conditions can be set as a clipped or a Flather's radiation
condition~\cite{flather76}. The bottom friction takes a semi-implicit
form for the sake of model stability. As done in the original version
of NEMO, a constant or Smagorinsky horizontal viscosity coefficient is
used for the horizontal viscosity term.

The traditional Arakawa C structured grid is employed here for the
discretisation of the computational domain. A two-dimensional integer
array is used to identify the different parts of the computational
domain; it has the value of one for ocean, zero for land and minus
one for ocean cells outside of the computational domain. This array
serves as a mark to identify ocean cells, land cells, solid boundaries
and open boundaries.

For the sake of simplicity, the explicit Eulerian forward time
stepping method is implemented here, except that the bottom friction
takes a semi-implicit form.  The Coriolis force can be set in explicit
or implicit form. The advection term is computed with a first-order
upwind scheme.

The order of the model computation is as follows:
\begin{enumerate}
\item Set the initial conditions (water depth, sea surface height, velocity)
\item integrate the continuity equation for the new sea surface height
\item update the different terms in the right hand side of the momentum equations (advection, Coriolis forcing(if set in explicit form), pressure gradient, and, horizontal viscosity
\item update the velocity vectors by summing up the values in 3), and implicitly with the bottom friction and Coriolis forcing (if set in implicit form)
\item apply the boundary conditions on to the open boundary and solid boundary cells.
\end{enumerate}

Since any real oceanographic computational model must output results,
we ensure that any \psykal version of nemolite2d retains the Input/Output
capability of the original. This aids in limiting the optimisations
that can be performed on the \psykal version to those that should also
be applicable to full oceanographic models. Note that although we
retain the I/O functionality, all of the results presented in this work
carefully exclude the effects of I/O since it is compute performance
that interests us here.

In order to maximise the flexibility (and thus potential for
optimisation) of the \psykal version of nemolite2d, we made the kernels
as fine-grained as possible. In this case, this resulted in eight
distinct kernels, each of which operated on a single field at a single
point (since we have chosen to use point-wise kernels). With a little
bit of tidying/re-structuring, we found it was possible to express the
contents of the main time-stepping loop as a single invoke (a call to
the PSy layer) and a call to the I/O system
(Figure~\ref{FIG_psykal_nemolite2d_structure}). The single PSy-layer
routine then consists of applying each of the kernels to all of the
points on the model mesh requiring an update. In its basic,
unoptimised (`vanilla') form, this PSy-layer routine then contains a
doubly-nested loop around each kernel call, as indicated by the
pseudo-code in Figure~\ref{FIG_psykal_nemolite2d_structure}.

\begin{figure}
\centering
%\includegraphics[width=85mm]{psykal_nemolite2d}
\caption{A schematic of the vanilla \psykal version of the nemolite2d code.}
\label{FIG_psykal_nemolite2d_structure}
\end{figure}

As with any full oceanographic model, boundary conditions must be
applied at the edges of the model domain. 

{\bf Auto-generated BC code by reasoning about meta-data. But then,
  that brings us on to meta-data...}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology(?)}
\label{sec_methodology}

Our aim in this work is to achieve portable performance. Consequently,
we have performed tests with the hardware and compilers listed in
Table~\ref{TABLE_compilers}. Where a compiler is available on a given
piece of hardware, the version number used in this work is specified.
The Intel Haswell-based system (Xeon E5-1620 v2) has a clock speed of
3.7~GHz and 10~MB of last-level cache. The Intel Ivy Bridge-based
system (Xeon E5-2697) has a clock speed of 2.7~GHz and a last-level
cache of 30~MB. 

We first describe the code transformations performed for the serial
version of nemolite2d.  We then move on to the construction of
parallel versions of the code using OpenMP and OpenACC. Again, we
describe the key steps we have taken in this process in order to
maximise the performance of the code.

\subsection{Transformations of Serial Nemolite2D}

% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{The matrix of compilers and hardware used in this work. The
  use of a compiler on a given system is indicated by the specification
  of the version of the compiler in the relevent element. No entry
  indicates that a compiler was not available/used on that system.}
\label{TABLE_compilers}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                 & \multicolumn{5}{c|}{Compiler}            \\
\hline
                 & Gnu   & Intel       & Cray    & IBM    &  PGI \\
\hline
Intel Haswell    & 4.8.2 & 14.0.0      &         &        &  \\
Intel Ivy Bridge & 4.9.1 & 15.0.0.090  & 8.3.3   &        &  \\
IBM Power 8      &       &             &         &        &  \\
NVIDIA Tesla K40 &       &             &         &        &  {\bf Jeremy} \\
\hline
\end{tabular}
\end{table}

In Table~\ref{TABLE_compiler_flags} we give the optimisation flags
used with each compiler. {\bf Explanation of the compiler flags?}
%These flags are particularly important for
%the \psykal versions of the code. The performance of the original
%version of the code is much less sensitive to the compiler
%options. This is because it consists of a single source file
%containing effectively a single routine (excluding those for I/O).
 
\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{The compiler flags used in this work.}
\label{TABLE_compiler_flags}
\centering
\begin{tabular}{l|c}
\hline
Compiler  &  Flags \\
\hline
Gnu       & -Ofast -mtune=native -finline-limit=50000    \\
Intel     & -O3 -fast -fno-inline-factor -xHost   \\
Cray      & -O3 -O ipa5 -h wp               \\
IBM       & -O5 -qinline=auto:level=10      \\
PGI       & {\bf Jeremy to supply} \\
\hline
\end{tabular}
\end{table}


Before applying any code transformations, we first benchmark the
original, serial version of the code. We also benchmark the vanilla,
unoptimised version after it has been re-structured following the
\psykal approach. In addition to this benchmarking, we profile these
versions of the code at the algorithm level (using a high-level timing
API). The resulting profiles are given in
Table~\ref{TABLE_profile}. The Momentum section dominates the profile
of both versions of the code, accounting for around 70--80\% of the
wall-clock time spent doing time-stepping. It is also the key section
when we consider the performance loss when moving from the original to
the \psykal version of the code; it slows down by a factor of
two. Although less significant in terms of absolute time, the
Continuity section is also dramatically slower in the \psykal version,
this time by a factor of three. In contrast, the performance of the
Time-update and Boundary Condition regions are not significantly
affected by the move to the \psykal version.

\begin{table}[!t]
\caption{The performance profile of the original and \psykal versions
  of nemolite2d (for 2000 time-steps of the $128^{2}$ domain and the
  Intel compiler).}
\label{TABLE_profile}
\centering
\begin{tabular}{l|c|c|c|c|c|c}
\hline
Section & \multicolumn{2}{c|}{Original} & \multicolumn{2}{c|}{Vanilla \psykal} &
\multicolumn{2}{c}{Final \psykal} \\
        & Time (s) & \%  & Time (s) & \%  & Time(s) & \% \\
\hline
Momentum    & 1.98  & 72.6  & 4.05  & 79.5 & 2.09 & 75.3 \\
Time-update & 0.40  & 14.6  & 0.41  & 8.1  & 0.29 & 10.6 \\
BCs         & 0.25  & 9.1   & 0.29  & 5.7  & 0.27 & 9.9  \\
Continuity  & 0.10  & 3.7   & 0.33  & 6.6  & 0.11 & 4.1  \\
\hline
\end{tabular}
\end{table}

Beginning with the vanilla \psykal version, we then apply a series of
code transformations while obeying the \psykal separation of concerns,
{\it i.e.} optimisation is restricted to the middle, {PS}y layer and
leaves the kernel and algorithm layers unchanged. The aim of these
optimisations is to recover, as much as is possible, the performance
of the original version of the code. The transformations we have
performed and the reasons for them are described in the following sections.

\subsubsection{Addition of `safe-address' directives}
\label{sec_safe_address}

Much of the optimisations we have performed have been informed by the
diagnostic output produced by either the Cray or Intel compilers. Many
of the nemolite2d kernels contain conditional statements. These
statements are there to check whether {\it e.g.} the current grid
point is wet or neighbours a boundary point. A compiler is better able
to optimise such a loop if it can be sure that all array accesses
within the body of the loop are safe for every trip, irrespective of
the conditional statements. In its diagnostic output the Cray compiler
notes this with messages of the form:
\begin{verbatim}
ftn-6375 ftn: VECTOR File = nemolite2d.f90,
 Line = 448 
A loop starting at line 448 would benefit 
from "!dir$ safe_address".
\end{verbatim}
In order that we could safely add such directives we altered the field
module to allocate all field-data arrays with extents greater than
strictly required.  We were then able to safely add the safe\_address
before all of the loops where the Cray compiler indicated it might be
useful (the Momentum loops and some of the BC loops).

\subsubsection{In-line Momentum kernel bodies into middle-layer code}
\label{sec_inline_mom}

The profiling data in Table~\ref{TABLE_profile} shows that it is the
Momentum section that accounts for the bulk of the model run-time.  We
therefore chose to attempt to optimise this section first. In-keeping
with the \psykal approach, we are only permitted to optimise the middle
(PSy) layer which, for this section comprises calls to two kernels,
one for each of the $x$ and $y$ components of momentum. These kernels
are relatively large; each comprises roughly 85 lines of Fortran
(excluding variable declarations).

From our previous work~\cite{shallow_psykal} on a similar code we know
that kernel in-lining is critical to obtaining performance with both
the Gnu and Intel compilers. For the Gnu compiler, this is because it
cannot perform in-lining when routines are in separate source
files. In our previous work we obtained an order-of-magnitude speed-up
simply by moving subroutines into the module containing the middle
layer (from which the kernels are called). A further performance
improvement of roughly 30\% was obtained when the kernel code was
manually inserted at the site of the subroutine call.

Although the Intel compiler does do in-lining when routines are in
separate source files, we have found (both here and in our previous
work~\cite{shallow_psykal}) that the number of optimisations it
performs is reduced if it first has to in-line a routine. For the
Intel-compiled Shallow code, manually inserting kernel code at the
site of the subroutine call increased performance by about 25\%.

In fact, in-lining can have a significant effect on the Intel
compiler's ability to vectorise a loop. Taking the loop that calls the
kernel for the u-component of momentum as an example, before in-lining
the compiler reports:
\begin{verbatim}
LOOP BEGIN at time_step_mod.f90(85,7) 
  inlined into nemolite2d.f90(86,11)
 remark #15335: loop was not vectorized: 
  vectorization possible but seems 
  inefficient. 
 --- begin vector loop cost summary ---
 scalar loop cost: 1307 
 vector loop cost: 2391.000 
 estimated potential speedup: 0.540 
 lightweight vector operations: 1133 
 medium-overhead vector operations: 1 
 vectorized math library calls: 1 
 --- end vector loop cost summary ---
LOOP END
\end{verbatim}
After we have manually in-lined the kernel body, the compiler reports:
\begin{verbatim}
LOOP BEGIN at time_step_mod.f90(97,7) 
  inlined into nemolite2d.f90(86,11)
 LOOP WAS VECTORIZED
 --- begin vector loop cost summary ---
 scalar loop cost: 1253 
 vector loop cost: 521.750 
 estimated potential speedup: 2.350 
 lightweight vector operations: 697 
 medium-overhead vector operations: 1 
 vectorized math library calls: 1 
 --- end vector loop cost summary ---
LOOP END
\end{verbatim}
Looking at the `estimated potential speedup' in the compiler outputs
above, it is clear that the way in which the compiler vectorises the
two versions must be very different. This conclusion is borne out by
the fact that if one persuades the compiler to vectorise the first
version (through the use of a directive) then the performance of the
resulting binary is worse than that where the loop is left
un-vectorised. {\bf In principle we could investigate this by looking
  at the assembler that the Intel compiler generates\ldots}

For the best possible performance, we have chosen to do full, manual
inlining for the two kernels making up the Momentum section.

\subsubsection{Force SIMD vectorisation of the Momentum kernels 
 using directives}

It turns out that the Cray-compiled binaries of both the original and
\psykal versions of nemolite2d perform considerably less well than
their Intel-compiled counterparts. Comparison of the diagnostic output
from each of the compilers revealed that while the Intel compiler was
happy to vectorise the Momentum loops, the Cray compiler was choosing
not to:
\begin{verbatim}
   99.          do ji = 2, M-1, 1
 A loop starting at line 99 was blocked 
 with block size 256.

 A loop starting at line 99 was not
 vectorized because it contains 
 conditional code which is more
 efficient if executed in scalar mode.
\end{verbatim}
Inserting the Cray `vector always' directive persuaded the compiler
to vectorise the loop:
\begin{verbatim}
  99.    !dir$ vector always
  100.          do ji = 2, M-1, 1
 A loop starting at line 100 was blocked
 with block size 256.

 A loop starting at line 100 requires an 
 estimated 17 vector registers at 
 line 151; 1 of these have been 
 preemptively forced to memory.

 A loop starting at line 100 was vectorized.
\end{verbatim}
which gave a significant performance improvement. This behaviour is in
contrast to that obtained with the Intel compiler: its predictions
about whether vectorising a loop would be beneficial were generally
found to be reliable.

\subsubsection{Work around limitations related to derived types}

Having optimised the Momentum section as much as permitted by the
\psykal approach, we turn our attention to the three remaining
sections of the code. The profile data in Table~\ref{TABLE_profile}
shows that these regions are all comparable in terms of cost. What is
striking however, is that the cost of the Continuity section increases
by more than a factor of three in moving to the \psykal version of the
code.

Comparison of the diagnostic output from the Cray and Intel
compilers revealed that the Cray compiler was vectorising the
Continuity section while the Intel compiler reported that it was
unable to do so due to dependencies. After some experimentation we
found that this was due to limitations in the compiler's analysis of
the way components of Fortran derived types were being used. Each
GOcean field object, in addition to the array holding the local
section of the field, contains a pointer to a GOcean grid object. If a
kernel requires grid-related quantities ({\it e.g.} the grid spacing)
then these are obtained by passing it a reference to the appropriate
array within the grid object. Although these grid-related quantities
are read-only within a compute kernel, if they were obtained from the
same object as that containing the array to which that kernel writes
then the Intel compiler identified a dependency preventing
vectorisation.  This limitation was simply removed by ensuring that
all read-only quantities were accessed via field objects that were
themselves read-only for the kernel at hand.  For instance, the call
to the continuity kernel, which confused the Intel compiler, originally
looked like this:
\begin{verbatim}
do jj = 2, N, 1
 do ji = 2, M, 1

   call continuity_code(ji, jj,      &
                        ssha%data,   &
                        sshn_t%data, &
                        ...,         &
                        ssha%grid%area_t)
\end{verbatim}
where {\it ssha} is the only field that is written to by the kernel.
We remove any potential confusion by instead obtaining the
grid-related (read-only) quantities from a field ({\it sshn\_t} in
this case) that is only read by the kernel:
\begin{verbatim}
do jj = 2, N, 1
 do ji = 2, M, 1

   call continuity_code(ji, jj,      &
                        ssha%data,   &
                        sshn_t%data, &
                        ...,         &
                        sshn_t%grid%area_t)
\end{verbatim}

\subsubsection{In-line the Continuity kernel}
\label{sec_cont_inline}

As with the Momentum kernel, we know that obtaining optimal
performance from both the Gnu and Intel compilers requires that a
kernel be manually in-lined at its call site. We do this for the
Continuity kernel in this optimisation step.

\subsubsection{In-line remaining kernels (BCs and time-update)}
\label{sec_inline_all_kernels}

Having optimised the Continuity section we finally turn our attention
to the Boundary Condition and Time-update sections. The kernels in
these sections are small and dominated by conditional statements.  We
therefore limited our optimisation of them to manually in-lining each
of the kernels into the PSy layer.

\subsubsection{In-line field-copy operations}
\label{sec_inline_fldcopy}

The Time-update section includes several array copies where fields for
the current time-step become the fields at the previous time-step.
Initially we implemented these copies as 'built-in' kernels (in the
GOcean library) as they are specified in the algorithm layer. However,
we obtained better performance by simply in-lining these array copies
into the PSy layer.

We shall see that the transformations we have just described do not
always result in improved performance. Whether or not they do so
depends both on the compiler used and the problem size. We also
emphasise that the aim of these optimisations is to make the PSy layer
as compiler-friendly as possible, following the lessons learned from
our previous work with the Shallow code~\cite{shallow_psykal}. It may
well be that transforming the code into some other structure would
result in better performance on a particular architecture. However,
exploring this optimisation space is beyond the scope of the present
work.

We explore the extent to which performance depends upon the problem
size by using square domains of dimension 64, 128, 256, 512 and 1024
for the traditional, cache-based CPU systems. This range allows us to
investigate what happens when cache is exhausted as well as giving us
some insight into the decisions that different compilers make when
optimising the code.

\subsection{Construction of OpenMP-Parallel Nemolite2D}
\label{sec_omp_steps}

For this part of the work we began with the optimal \psykal version of
the code, as obtained after applying the various transformations
described in the previous section. As with the transformations of the
serial code, our purpose here is to determine the functionality
required of the PSyclone code-generation system.

\subsubsection{Separate PARALLEL DOs}
\label{sec_sep_dos}

The simplest possible OpenMP-parallel implementation consists of
parallelising each loop nest in the PSy layer. This was done by
inserting an OpenMP PARALLEL DO directive before each loop nest so
that the iterations of the outermost or $j$ loop (over the y-dimension
of the model domain) are shared amongst the OpenMP threads. This
leaves the innermost ($i$) loop available for SIMD vectorisation by
the compiler.

The loop nest dealing with the application of the Flather boundary
condition to the y-component of velocity has a loop-carried dependency
in $j$ which prevented its being executed in parallel. This was
therefore left unchanged and executed on thread 0 only.

\subsubsection{Single PARALLEL region}
\label{sec_single_parallel}

Although very simple to implement, the use of separate PARALLEL DO
directives causes the team of OpenMP threads to be repeatedly created
and destroyed. This may be avoided by keeping the thread team in
existence for as long as possible using an OpenMP PARALLEL region.  We
therefore enclosed the whole of the PSy layer (in this code, a single
subroutine) within a single PARALLEL region. The directive preceeding
each loop nest to be parallelised was then changed to an OpenMP DO. We
ensured that the v-Flather loop nest was executed in serial (by the
first thread to encounter it) by enclosing it within an OpenMP SINGLE
section.

\subsubsection{First-touch policy}
\label{sec_1sttouch}

When executing an OpenMP-parallel program on a Non-Uniform Memory
Access (NUMA) compute node it becomes important to ensure that the
memory locations accessed by each thread are local to the hardware
core upon which it is executing. One way of doing this is to implement
a so-called 'first-touch policy' whereby memory addresses that will
generally be accessed by a given thread during program execution are
first initialised by that thread. This is simply achieved by using an
OpenMP-parallel loop to initialise newly-allocated arrays to some
value, {\it e.g.} zero.

Since data arrays are managed within the GOcean Library {\bf (is this
  a good/appropriate name?)} this optimisation can again be
implemented without changing user code (the Application and Kernel
layers).

\subsubsection{Minimise thread synchronisation}
\label{sec_synch}

By default, the OpenMP END DO directive includes an implicit barrier,
thus causing all threads to wait until the slowest has completed the
preceeding loop. Such synchronisation limits performance at larger
thread counts and, for the nemolite2d code, is frequently unecessary.
{\it E.g.} if a kernel does not make use of the results of a
preceeding kernel call then there is clearly no need for threads to
wait between the two kernels.

We analysed the inter-dependencies of each of the code sections within
the PSy layer and removed all unecessary barriers by adding the NOWAIT
qualifier to the relevant OpenMP END DO or END SINGLE directives. This
reduced the number of barriers from eleven down to four.

\subsubsection{Amortize serial region}
\label{sec_amortize}

As previously mentioned, the v-Flather section has to be executed in
serial becuase of a loop-carried dependence in $j$. (In principle we
could choose to parallelise the inner, $i$ loop but that would inhibit
its SIMD vectorisation.) This introduces a load-imbalance between the
threads. We attempt to mitigate this by moving this serial section to
before the (parallel) u-Flather section. Since these two sections are
independent, the aim is that the thread that performs the serial,
v-Flather computation then performs a smaller share of the following
u-Flather loop. In practice, this requires that some form of dynamic
thread scheduling is used.

\subsubsection{Thread scheduling}
\label{sec_scheduling}

In order to investigate how thread scheduling affects performance we
used the `runtime' argument to the OpenMP SCHEDULE qualifier for all
of our OpenMP parallel loops. The actual schedule to use can then be
set at run-time using the OMP\_SCHEDULE environment variable. We
experimented with using the standard static, dynamic and guided (with varying
chunk size) OpenMP schedules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

We first consider the performance of the code in serial and examine
the effects of the transformations described in
Section~\ref{sec_methodology}. Once we have arrived at an optimal form
for the serial version of nemolite2d we then investigate its parallel
performance on both CPU- and GPU-based systems.

\subsection{Serial Performance}

In Figure~\ref{FIG_orig_perf_summary} we plot the serial performance
of the original version of the nemolite2d code for the range of
compilers and CPUs considered here. Unlike the Shallow code, the
original version of nemolite2d has not been optimised. Although it is
still a single source file it is, in common with NEMO itself,
logically structured with separate subroutines performing different
parts of the physics within each time step. This structuring and the
heavy use of conditional statements favour the Intel compiler which
significantly out-performs both the Gnu and Cray compilers. Only when
the problem size falls out of cache does the performance gap begin to
narrow. The reason for the performance deficit of the Cray-compiled
binary comes down to SIMD vectorisation, an issue that we explore
below.

\begin{figure}[!t]
\centering
\includegraphics[width=2.8in]{orig_summary}
\caption{Summary of the performance of the original version of the 
nemolite2d code on 
the range of compilers and CPU-based systems under consideration.}
\label{FIG_orig_perf_summary}
\end{figure}

Moving now the to the \psykal version of nemolite2d,
Figure~\ref{FIG_best_psykal_perf_summary} plots the performance of the
fastest \psykal version for each of the compiler/CPU combinations.
While the Intel compiler still produces the best-performing binary,
the Cray-compiled binary is now a very close second. In fact, the
performance of both the Gnu- and Cray-compiled \psykal versions is
generally significantly greater than that of their respective original
versions. We also note that best absolute performance (in terms of
grid points processed per second) with any compiler is obtained with
the $256^2$ domain.

\begin{figure}[!t]
\centering
\includegraphics[width=7.5cm]{best_psykal_summary}
\caption{Summary of the best performance achieved by any \psykal 
version of nemolite2d with each of the compilers and CPUs under 
consideration.}
\label{FIG_best_psykal_perf_summary}
\end{figure}

Figure~\ref{FIG_slowdown_summary} plots the percentage difference
between the performance of the original and the best \psykal versions
of nemolite2d for each compiler/CPU combination. This shows that it is
only the Intel-compiled binary running the $64^2$ domain that is
slower with the \psykal version of the code (and then only by some
3\%). For all other points in the space, the optimised \psykal version
of the code performs better. The results for the Cray compiler are
however somewhat sqewed by the fact that it did not SIMD vectorise key
parts of the original version (see below).

\begin{figure}[!t]
\centering
\includegraphics[width=2.8in]{slowdown_summary}
\caption{Comparison of the performance of the best \psykal
version with that of the original version of the code. A negative value 
indicates that the \psykal version is slower than the original.}
\label{FIG_slowdown_summary}
\end{figure}

Having show that we can recover, and often improve upon, the
performance of the original version of nemolite2d, the next logical
step is to examine the necessary code transformations in detail.  We
do this for the $256^{2}$ case since this fits within cache on the Ivy
Bridge CPUs we are using here.  Table~\ref{TABLE_opt_breakdown} shows
detailed performance figures for this case after each transformation has been
applied to the code. The same data is visualised in
Figure~\ref{FIG_opt_stages_256} where the performance of the \psykal
version for a given compiler/CPU is given relative to the performance
of the original with the same compiler/CPU.

Looking at the results for the Gnu compiler (and the Ivy Bridge CPU)
first, all of the steps-up in performance correspond to kernel
in-lining. None of the other transformations had any effect on the
performance of the compiled code. In fact, simply in-lining the two
kernels associated with the Momentum section was enough to improve
upon the performance of the original code. 

With the Intel compiler, the single largest performance increase is
again due to kernel in-lining (of the Momentum kernels). This is
because the compiler does a much better job of SIMD vectorising the
loops involved than it does when it first has to in-line the kernel
itself (as evidenced by its own diagnostic output - see
Section~\ref{sec_inline_mom}). However, although this gives a
significant performance increase it is not sufficient to match the
performance of the original version. This is only achieved by
in-lining every kernel and making the lack of data dependencies
between arrays accessed from different field objects more explicit.

The Cray compiler is distinct from the other two in that kernel
in-lining does not give any performance benefit and in fact, for the
smaller kernels, it can actually hurt performance. Thus the key
transformation is to encourage the compiler to SIMD vectorise the
Momentum section via a compiler-specific directive (without this it
concludes that such vectorisation would be inefficient). Of the other
transformations, only the addition of the compiler-specific
safe\_address directive (see Section~\ref{sec_safe_address}) was found
to give any benefit.

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Performance (millions of points updated per second) for the
  $256^2$ case after each code transformation. Where an optimisation
  utilises compiler-specific directives then performance figures for
  the other compilers are omitted.}
\label{TABLE_opt_breakdown}
\centering
\begin{tabular}{l|c|c|c}
\hline
Compiler:           & Gnu & Intel & Cray  \\
\hline
CPU:                & \multicolumn{3}{c}{Ivy Bridge}  \\
\hline
Original         & 3.87 & 12.2 & 6.83 \\
Vanilla \psykal  & 3.07 & 6.55 & 6.73 \\
Safe-address     & --   & --   & 6.95 \\
In-line Momentum & 4.31 & 10.7 & 6.93 \\
SIMD Momentum    & --   & --   & 11.8 \\
\parbox{2.5cm}{\raggedright Grid data from read-only objects} & 4.31 & 11.3 & 11.8 \\
In-line Continuity         & 4.83 & 11.8 & 11.6 \\
In-line remaining kernels  & 5.89 & 12.0 & 11.5 \\
In-line field copies       & 5.92 & 12.5 & 11.4 \\
\hline
\parbox{2.5cm}{\raggedright \%-speed-up of best {\it c.f.} original} & 34.62 & 2.39 & 42.22 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=90mm]{opt_stages_256}
\caption{Performance of the \psykal version of nemolite2d for the
  $256^{2}$ domain at each stage of optimisation. The first (black)
  bar of each cluster gives the performance of the original version of
  nemolite2d for that compiler/CPU combination.}
\label{FIG_opt_stages_256}
\end{figure}

% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.

%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.


\subsection{Parallel Performance}

We now turn to transformations related to parallelisation of the
nemolite2d code; the introduction of OpenMP and OpenACC directives. In
keeping with the \psykal approach we do not modify either the
Algorithm- or Kernel-layer code.  Any code changes are restricted to
either the PSy (middle) layer or the underlying library that manages
e.g. the construction of field objects.

\subsubsection{OpenMP Version}

As with the serial optimisations, we consider the effect of each of
the OpenMP optimisation steps described in Section~\ref{sec_omp_steps}
for the $256^2$ domain. For this we principally use a single Intel,
Ivy Bridge socket which has 12 hardware cores and support for up to 24
threads with hyperthreading (i.e. two threads per core).
Figures~\ref{FIG_gnu_omp_256},~\ref{FIG_intel_omp_256}
and~\ref{FIG_cray_omp_256} show the performance of each of the
versions of the code on this system for the Gnu, Intel and Cray
compilers, respectively.

In order to quantify the scaling behaviour of the different versions
of nemolite2d with the different compilers/run-time environments, we
also plot the parallel efficiency in
Figures~\ref{FIG_gnu_omp_256},~\ref{FIG_intel_omp_256}
and~\ref{FIG_cray_omp_256} (dashed lines and open symbols). We define
parallel efficiency (\%), $E(n)$, on a number of threads, $n$, as:
\[
E(n) = 100  \frac{P(n)}{ n P(1)}
\]
where $P(m)$ is the performance of the code on $m$ threads. For a
perfect, linearly-scaling code, $E(n)$ will be 100\%.

Since the space to explore consists of three different compilers, six
different domain sizes, five stages of optimisation and six different
thread counts we can only consider a slice through it in what follows.
In order to inform our choice of domain size,
Figure~\ref{FIG_omp_domain_size} shows the scaling behaviour of the
most performant, Intel-compiled version of nemolite2d. Although it is
common for production runs of NEMO to use MPI sub-domains of $20
\times 20$ ({\bf do we have/need a reference for that?}), it is clear
from Figure~\ref{FIG_omp_domain_size} that the limited quantity of
parallelism in the $32 \times 32$ domain inhibits scaling. Therefore,
to fully test the performance of our OpenMP implementations we have
chosen to examine results for the $256 \times 256$ domain and these
are shown in Figures~\ref{FIG_gnu_omp_256}, \ref{FIG_intel_omp_256}
and \ref{FIG_cray_omp_256}.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_problem_size}
\caption{The scaling behaviour of the most performant OpenMP-parallel
  version of \psykal nemolite2d for the full range of domain sizes
  considered on the CPU. Results are for the Intel compiler on a
  single Intel Ivy Bridge socket. The corresponding parallel
  efficiencies are shown using open symbols and dashed lines. The
  24-thread runs employed hyperthreading.}
\label{FIG_omp_domain_size}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_256_gnu}
\caption{Performance of the OpenMP-parallel version of \psykal
  nemolite2d for the Gnu compiler on a single Intel Ivy Bridge
  socket. The corresponding parallel efficiencies are shown using open
  symbols and dashed lines. The 24-thread runs employed hyperthreading
  and the optimal OpenMP schedule is given in parentheses.}
\label{FIG_gnu_omp_256}
\end{figure}

The simplest OpenMP implementation (black lines, circle symbols) fails
to scale well for any of the compilers. For the Intel and Gnu
versions, parallel efficiency is already less than 50\% on just four
threads. The Cray version however does better and is about 45\%
efficient on eight threads.

With the move to a single PARALLEL region, the situation
is greatly improved with all three executables now scaling out to at
least 12 threads with $\sim70$\% efficiency (red lines, square symbols).

In restricting ourselves to a single socket, we are keeping all
threads within a single NUMA region. It is therefore surprising that
implementing a `first-touch' policy has any effect and yet, for the
Gnu- and Intel-compiled binaries, it appears to improve performance
when hyperthreading is employed to run on 24 threads (green lines and
diamond symbols in Figures~\ref{FIG_gnu_omp_256} and
~\ref{FIG_intel_omp_256}). {\bf Can we explain this?}

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_256_intel}
\caption{Performance of the OpenMP-parallel version of \psykal
  nemolite2d for the Intel compiler on a single Intel Ivy Bridge
  socket.  The corresponding parallel efficiencies are shown using
  open symbols and dashed lines. The 24-thread runs employed
  hyperthreading and the optimal OpenMP schedule is given in
  parentheses.}
\label{FIG_intel_omp_256}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_256_cray}
\caption{Performance of the OpenMP-parallel version of \psykal
  nemolite2d for the Cray compiler on a single Intel Ivy Bridge
  socket.  The corresponding parallel efficiencies are shown using
  open symbols and dashed lines. The 24-thread runs employed
  hyperthreading and the optimal OpenMP schedule is given in
  parentheses.}
\label{FIG_cray_omp_256}
\end{figure}

The final optimisation step that we found to have any significant
effect is to minimise the amount of thread synchronisation by
introducing the NOWAIT qualifier wherever possible (blue lines and
upwards-triangle symbols). For the Gnu compiler, this improves the
performance of the executable on eight or more threads while, for the
Intel compiler, it only gives an improvement for the 24-thread
case. Moving the SINGLE region before a parallel loop is marginally
beneficial for the Gnu- and Cray-compiled binaries and yet reduces the
performance of the Intel binary (purple lines and right-pointing
triangles).

We have used different scales for the $y$-axes in each of the plots in
Figures~\ref{FIG_gnu_omp_256},~\ref{FIG_intel_omp_256}
and~\ref{FIG_cray_omp_256} in order to highlight the performance
differences between the code versions with a given compiler. However,
the best performance obtained on twelve threads ({\it i.e.}  without
hyperthreading) is 52.1, 55.6 and 46.3 (million points/s) for the Gnu,
Intel and Cray compilers, respectively. This serves to emphasise the
democratisation that the introduction of OpenMP has had; for the
serial case the Cray- and Intel-compiled exectables were a factor of
two faster than the Gnu-compiled binary
(Figure~\ref{FIG_opt_stages_256}). In the OpenMP version, the
Gnu-compiled binary is only 6\% slower than that produced by the Intel
compiler and is 13\% {\em faster} that that of the Cray compiler. In
part this situation comes about because of the effect that adding the
OpenMP compiler flag has on the optimisations performed by the
compiler.  In particular, the Cray compiler no-longer vectorises the
key Momentum section, despite the directive added during the serial
optimisation work.

Since NEMO and similar finite-difference codes tend to be
memory-bandwidth bound, we checked the sensitivity of our performance
results to this quantity by benchmarking using two sockets of Intel
Ivy Bridge ({\it i.e.} using a complete node of Archer). For this
configuration we ensured that threads were evenly shared over the two
sockets. The performance obtained for the $256^2$ case with the `early
SINGLE' version of the code is compared with the single-socket
performance in Figure~\ref{FIG_omp_2socks}. Surprisingly, doubling the
available memory bandwidth in this way has little effect on
performance - the two-socket performance figures track those from a
single socket very closely. The only realy difference in performance
is at 24 threads where, in addition to the difference in available
memory bandwidth, the single-socket configuration is using
hyperthreading while the two-socket case is not.  {\bf Have query in
  with Archer helpdesk on behaviour seen with Cray compiler here. Also
  would like to run 512 case on 2 sockets to see how it goes out to 48
  threads.}

\begin{figure}
\centering
\includegraphics[width=\picwidth]{1and2sockets}
\caption{Performance of the OpenMP-parallel version of \psykal
  nemolite2d on one and two sockets of Intel Ivy Bridge. The 24-thread
  runs on a single socket used hyperthreading and the two-socket runs
  had the threads shared equally between the sockets.}
\label{FIG_omp_2socks}
\end{figure}

A further complication is the choice of scheduling of the OpenMP
threads.  We have investigated the performance of each of the
executables (and thus the associated OpenMP run-time library) with the
standard OpenMP {\it static}, {\it dynamic} and {\it guided}
scheduling policies~\cite{openmp_standard}. For the Intel
compiler/run-time, static loop scheduling was found to be best for all
versions apart from that where we have attempted to amortize the cost
of the SINGLE section. This is to be expected since that strategy
requires some form of dynamic loop scheduling in order to reduce the
load imbalance introduced by the SINGLE section.

In contrast, some form of dynamic scheduling gave a performance
improvement with the Gnu compiler/run-time even for the `first-touch'
version of the code. This is despite the fact that this version
contains (implicit) thread synchronisation after every parallel loop.
For the Cray compiler/run-time, some form of dynamic scheduling became
optimal once inter-thread synchronisation was reduced using the NOWAIT
qualifiers.


%\begin{figure}
%\centering
%\includegraphics[width=\picwidth]{omp_scaling_32_gnu}
%\caption{Performance of the OpenMP-parallel version of \psykal
%  nemolite2d for the Gnu compiler on a single Intel Ivy Bridge
%  socket. The corresponding parallel efficiencies are shown using open
%  symbols and dashed lines. The 24-thread runs employed hyperthreading
%  and the optimal OpenMP schedule is given in parentheses.}
%\label{FIG_gnu_omp_32}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=\picwidth]{omp_scaling_32_intel}
%\caption{Performance of the OpenMP-parallel version of \psykal
%  nemolite2d for the Intel compiler on a single Intel Ivy Bridge
%  socket.  The corresponding parallel efficiencies are shown using
%  open symbols and dashed lines. The 24-thread runs employed
%  hyperthreading and the optimal OpenMP schedule is given in
%  parentheses.}
%\label{FIG_intel_omp_32}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=\picwidth]{omp_scaling_32_cray}
%\caption{Performance of the OpenMP-parallel version of \psykal
%  nemolite2d for the Cray compiler on a single Intel Ivy Bridge
%  socket.  The corresponding parallel efficiencies are shown using
%  open symbols and dashed lines. The 24-thread runs employed
%  hyperthreading and the optimal OpenMP schedule is given in
%  parentheses.}
%\label{FIG_cray_omp_32}
%\end{figure}

\subsubsection{OpenACC Version}

The introduction of OpenACC directives proved to be simpler than
producing the OpenMP version of nemolite2d {\bf - Jeremy to clarify
  whether this is true}.  Consequently we only show the performance of
{\bf xxx} versions of the code in Figure~\ref{FIG_gpu_opt_stages}.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{gpu_opt_stages}
\caption{Performance of the various GPU implementations of nemolite2d.}
\label{FIG_gpu_opt_stages}
\end{figure}

In Figure~\ref{FIG_cpu_cf_gpu} we compare the absolute performance of
the OpenMP and OpenACC implementations of nemolite2d across the range
of problem sizes considered. The OpenMP figures are the maximum
performance obtained from a whole Intel Ivy Bridge socket by any
version of the code on any number of threads for a given
compiler/run-time. For the smallest domain size ($64^2$) the OpenMP
version significantly outperforms the GPU because there is
insufficient parallelism to fully utilise the GPU and one time-step
takes only 80~$\mu$s. The execution of a single time-step is then
dominated by the time taken to launch a kernel on the GPU rather than
the execution of the kernel itself.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{cpu_cf_gpu}
\caption{Performance of the best OpenMP-parallel version of \psykal
  nemolite2d (on a single Intel Ivy Bridge socket) compared with the best GPU implementation. {\bf This should really be best OpenACC version as that's what PSyclone can support.}}
\label{FIG_cpu_cf_gpu}
\end{figure}

Once the problem size is increased to $128^2$, a single time-step
takes roughly 200~$\mu$s and only the Intel-compiled OpenMP version is
comparable in performance to the OpenACC version. For all of the
larger problem sizes plotted in Figure~\ref{FIG_cpu_cf_gpu} the GPU
version is considerably faster than the CPU.  For problem sizes of
$1024^2$ and greater, the 30MB cache of the Ivy Bridge CPU is
exhausted and performance becomes limited by the bandwidth to main
memory. At this stage the OpenACC version of the code on the GPU is
some {\bf 3.7 (this is the factor for the CUDA version!)} times faster
than the OpenMP version on the CPU. {\bf Can we relate this to the
  difference in peak memory b/w between the GPU and CPU?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

We have investigated the application of the \psykal separation of
concerns approach to the domain of finite-difference ocean
models. This approach enables the computational science (performance)
related aspects of a computer model to be kept separate from the
natural (oceanographic) science aspects. In this work we have shown
that the \psykal approach has the potential to improve performance and
to improve code structure/maintainability.

The application of code transformations to the middle/PSy layer is key
to the performance of the \psykal version of a code.




% use section* for acknowledgement
\section*{Acknowledgments}

This work made use of the ARCHER UK National Supercomputing Service
(\url{http://www.archer.ac.uk}) and some {\bf NVIDIA system}.

% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,nemolite2d_perf}
%

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiographynophoto}{A.~R.~Porter}
Biography text here.
\end{IEEEbiographynophoto}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{R.~Ford}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{G.~Riley}
Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}


% that's all folks
\end{document}



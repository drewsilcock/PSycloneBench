%% nemolite2d_perf.tex
%%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.

\usepackage{cite}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.eps}
\fi


% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
\usepackage{dblfloatfix}


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.

\usepackage{url}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\newlength{\picwidth}
\setlength{\picwidth}{85mm}

\newcommand{\psykal}{{PS}y{KA}l}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Something about Performance, 2D Component of NEMO. (TBC)}
%
% author names and IEEE memberships
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{A.~R.~Porter, J.~Appleyard, M.~Ashworth, R.~W.~Ford, J.~Holt, H.~Liu and G.~D.~Riley% <-this % stops a space
\thanks{M.~Ashworth, R.~W.~Ford and A.~R.~Porter are with the Science and Technology Facilities Council, UK.}% <-this % stops a space
\thanks{J.~Appleyard is with NVIDIA.}%
\thanks{H.~Liu and J.~Holt are with the National Oceanography Centre, Liverpool, UK.}%
\thanks{G.~Riley is with the University of Manchester, Manchester, UK.}% <-this % stops a space
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names.


% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~11, No.~4, December~2012}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2012 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
The abstract goes here.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Performance, Code-generation, Finite-difference, NEMO, OpenMP, OpenACC
\end{IEEEkeywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HE" in caps to complete the first word.
\IEEEPARstart{T}{he} challenge presented to the developers of
scientific software by the drive towards Exascale computing is
considerable. With power consumption becoming the overriding design
constraint, CPU clock speeds are falling and the complex,
multi-purpose compute core is being replaced by multiple, simpler
cores. This philosophy can be seen at work in the rise of so-called
accelerator-based machines in the Top 500 List~\cite{top500} of
supercomputers: four of the top-ten machines in the June 2016 list
make use of many-core processors (Intel Xeon Phi, NVIDIA GPU or NRCPC
SW26010). Two of the remaining six machines are IBM BlueGene/Qs, the
CPU of which has hardware support for running 64 threads.

Achieving good performance on large numbers of light-weight cores
requires exploiting as much parallelism in an application as possible
and this results in increased complexity in the programming models
that must be used. This in turn increases the burden of code
maintenance and code development, in part because two specialisms are
required: that of the scientific domain which a code is modelling
({\it e.g.} oceanography) and that of computational science. The
situation is currently complicated still further by the existence of
competing hardware technology; if one was to begin writing a major
scientific application today it is unclear whether one would target
GPU, Xeon Phi, traditional CPU, FPGA or something else entirely. This
is a problem because, generally speaking, these different technologies
require different programming approaches.

In a previous paper~\cite{shallow_psykal} we introduced a possible
approach to tackling this problem which we term \psykal\ (discussed
below). In that work we considered the implications for serial
performance of the extensive code re-structuring required by the
approach when applied to a shallow-water model. We found that although
the re-structuring did initially incur a sizeable performance penalty,
it was possible to transform the resulting code to recover performance
while obeying the \psykal\ separation of concerns. In this work we
move to looking at parallel performance within the \psykal\ approach.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\subsection{The \psykal\ Approach}

The \psykal\ approach attempts to address the problems described in
the previous section. It separates code into three layers; the
Algorithm layer, the PSy layer and the Kernel layer. The approach has
been developed in the GungHo project~\cite{GungHo}, which is creating
a new Dynamical core for the UK Met Office, and its design has been
influenced by earlier work on OP2~\cite{OP2, PYOP2}.

In common with OP2, the \psykal\ approach separates out the science
code and the performance-related code into distinct layers. The calls
that specify parallelism in both approaches are similar in terms of
where they are placed in the code and in their semantics. However, the
\psykal\ approach supports the specification of more than one kernel
in a parallel region of code, compared with one for OP2, giving more
scope for optimisation. Another difference is that the description
metadata is provided with a kernel in the \psykal\ approach whereas it
is provided as part of the kernel call in OP2.

While the \psykal\ approach is general, we are currently applying it to
Atmosphere and Ocean models written in Fortran where domain
decomposition is typically performed in the latitude-longitude
dimension, leaving columns of elements on each domain-decomposed
partition.

The top layer, in terms of calling hierarchy, is the Algorithm
layer. This layer specifies the algorithm that the scientist would like
to perform (in terms of calls to kernel and infrastructure routines)
and logically operates on full fields. We say logically here as the
fields may be domain decomposed, however the Algorithm layer is not
aware of this. It is the scientist's responsibility to write this
Algorithm layer.

The bottom layer, in terms of calling hierarchy, is the Kernel
layer. The Kernel layer implements the science that the Algorithm
layer calls, as a set of subroutines. These kernels operate on fields
that are local to the process doing the computation. (Depending on the
type of kernel, these may be a set of elements, a single column of
elements, or a set of columns.). Again the scientist is responsible
for writing this layer and there is no parallelism specified here,
but, depending on the complexity of the Kernels, there may be input
from an HPC expert and/or some coding rules to help ensure the kernels
compile into efficient code.  In an alternative approach, kernels are
generated from a high-level specification~\cite{firedrake,fenics},
potentially allowing them to be optimised automatically~\cite{coffee}.

The PSy layer sits in-between the Algorithm and Kernel layers and its
functional role is to link the algorithm calls to the associated
kernel subroutines. As the Algorithm layer works on logically global
fields and Kernel layer works on local fields, the PSy layer is
responsible for iterating over columns. It is also responsible for
including any distributed-memory operations resulting from the
decomposition of the simulation domain, such as halo swaps and
reductions.

As the PSy layer iterates over columns, the single core performance
can be optimised by applying transformations such as loop fusion and
loop blocking. Additionally, the potential parallelism within this
iteration space can also be exploited and optimised. The PSy layer can
therefore be tailored for a particular hardware (such as multi-core,
many-core, GPGPUs, or some combination thereof) and software (such as
compiler, operating system, MPI implementation) configuration with no
change to the Algorithm or Kernel layer code. This approach therefore
offers the potential for portable performance. In this work we apply
optimisations to the PSy layer manually. The development of a tool to
automate this process will be the subject of a future paper.

Clearly the separation of code into distinct layers may have an effect
on performance. This overhead, how to get back to the performance
of a hand-optimised code, and potentially improve on it, will be
discussed in the remainder of this paper.

\subsection{The `NEMOLite2D' Program}

For this work we have used a program, `NEMOLite2D,' developed by
ourselves.  NEMOLite2D is a vertically-averaged version of NEMO
(Nucleus for European Modelling of the Ocean~\cite{nemo}), retaining
only its dynamical part. The whole model system is represented through
one continuity equation \eqref{continuity} (for the update of the
sea-surface height) and two vertically-integrated momentum equations
\eqref{momentum} (for the two velocity components, respectively).

\begin{equation}\label{continuity}
 \frac{\partial \zeta}{\partial t} + \vec{\nabla} \cdot (\vec{U}h) = 0
\end{equation}
\begin{equation}\label{momentum}
 \frac{\partial \vec{U}h}{\partial t}
   + \vec{U} \cdot \vec{\nabla} (\vec{U}h) = 
   - gh \vec{\nabla} \zeta - 2h \vec{\Omega} \times \vec{u}
   + \nu h \Delta \vec{U}
\end{equation}

where $\zeta$ and $\vec{U}$ represent the sea-surface height and
horizontal velocity vector, respectively. $h$ is the total water
depth, $\Omega$ is the Earth rotation velocity vector.  $g$ is the
acceleration due to gravity. $\nu$ is the kinematic viscosity coefficient.

The external forcing includes surface wind stress, bottom friction,
and open-boundary barotropic forcing. A lateral-slip boundary
condition is applied along the coast lines. The open boundary
condition can be set as a clipped or Flather's radiation
condition~\cite{flather76}. The bottom friction takes a semi-implicit
form for the sake of model stability. As done in the original version
of NEMO, a constant or Smagorinsky horizontal viscosity coefficient is
used for the horizontal viscosity term.

The traditional Arakawa C structured grid is employed here for the
discretisation of the computational domain. A two-dimensional integer
array is used to identify the different parts of the computational
domain; it has the value of one for ocean, zero for land and minus
one for ocean cells outside of the computational domain. This array
enables the identification of ocean cells, land cells, solid boundaries
and open boundaries.

For the sake of simplicity, the explicit Eulerian forward time
stepping method is implemented here, except that the bottom friction
takes a semi-implicit form.  The Coriolis force can be set in explicit
or implicit form. The advection term is computed with a first-order
upwind scheme.

The order of the model computation is as follows:
\begin{enumerate}
\item Set the initial conditions (water depth, sea surface height, velocity)
\item integrate the continuity equation for the new sea surface height
\item update the different terms in the right hand side of the momentum equations; advection, Coriolis forcing (if set in explicit form), pressure gradient, and horizontal viscosity
\item update the velocity vectors by summing up the values in 3), and implicitly with the bottom friction and Coriolis forcing (if set in implicit form)
\item apply the boundary conditions on the open-  and solid-boundary cells.
\end{enumerate}

Since any real oceanographic computational model must output results,
we ensure that any \psykal\ version of NEMOLite2D retains the Input/Output
capability of the original. This aids in limiting the optimisations
that can be performed on the \psykal\ version to those that should also
be applicable to full oceanographic models. Note that although we
retain the I/O functionality, all of the results presented in this work
carefully exclude the effects of I/O since it is compute performance
that interests us here.

In order to maximise the flexibility (and thus potential for
optimisation) of the \psykal\ version of NEMOLite2D, we made the
kernels as fine-grained as possible. This resulted in eight distinct
kernels, each of which updates a single field at a single point (since
we have chosen to use point-wise kernels). With a little bit of
tidying/re-structuring, we found it was possible to express the
contents of the main time-stepping loop as a single invoke (a call to
the PSy layer) and a call to the I/O system
(Figure~\ref{FIG_psykal_nemolite2d_structure}). The single invoke
gives us a single PSy-layer routine which consists of applying each of
the kernels to all of the points requiring an update on the model
mesh. In its basic, unoptimised (`vanilla') form, this PSy-layer
routine then contains a doubly-nested loop around each kernel call.

\begin{figure}
\centering
\begin{verbatim}
DO istep = 1, nsteps
  call invoke( continuity(ssha_t, ...), &
               momentum_u(ua, un, ...), &
               ...,                     &
               next_sshu(sshn_u, ...),  &
               next_sshv(sshn_v, ...) )
  call model_write(istep, sshn_t, un, vn)
END DO
\end{verbatim}
\caption{A schematic of the top-level of the \psykal\ version of the
 NEMOLite2D code.}
\label{FIG_psykal_nemolite2d_structure}
\end{figure}

As with any full oceanographic model, boundary conditions must be
applied at the edges of the model domain. Since NEMOLite2D applies
external boundary conditions ({\it e.g.} barotropic forcing), this is
done via user-supplied kernels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec_methodology}

Our aim in this work is to achieve portable performance. Consequently,
we have performed tests with the hardware and compilers listed in
Table~\ref{TABLE_compilers}. Where a compiler is available on a given
piece of hardware, the version number used in this work is specified.
%The Intel Haswell-based system (Xeon E5-1620 v2) has a clock speed of
%3.7~GHz and 10~MB of last-level cache. 
The Intel Ivy Bridge-based
system (Xeon E5-2697) has a clock speed of 2.7~GHz and a last-level
cache of 30~MB. 

We first describe the code transformations performed for the serial
version of NEMOLite2D.  We then move on to the construction of
parallel versions of the code using OpenMP and OpenACC. Again, we
describe the key steps we have taken in this process in order to
maximise the performance of the code. In both cases our aim is to
identify those transformations which must be supported by any tool
which seeks to auto-generate a performant PSy layer.

\subsection{Transformations of Serial NEMOLite2D}

% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
%
\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{The matrix of compilers and hardware used in this work. The
  use of a compiler on a given system is indicated by the specification
  of the version of the compiler in the relevent element. No entry
  indicates that a compiler was not available/used on that system.}
\label{TABLE_compilers}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                 & \multicolumn{5}{c|}{Compiler}            \\
\hline
                 & Gnu   & Intel       & Cray    & IBM    &  PGI \\
\hline
%Intel Haswell    & 4.8.2 & 14.0.0      &         &        &  \\
Intel Ivy Bridge & 4.9.1 & 15.0.0.090  & 8.3.3   &        &  \\
IBM Power 8      &       &             &         &        &  \\
NVIDIA Tesla K40 &       &             &         &        &  {\bf Jeremy} \\
\hline
\end{tabular}
\end{table}

In Table~\ref{TABLE_compiler_flags} we give the optimisation flags
used with each compiler. For the IBM, Gnu and Intel compilers, we
include flags to encourage in-lining of kernel bodies. The Intel flag
``-xHost'' enables the highest-level of SIMD vectorisation supported
by the host CPU (AVX in this case). The Intel flag ``-fast'' and Cray
flags ``-O ipa5'' and ``-h wp'' enable inter-procedural optimisation.

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{The compiler flags used in this work.}
\label{TABLE_compiler_flags}
\centering
\begin{tabular}{l|l|l}
\hline
Compiler  &  Flags                                    & Notes \\
\hline
Gnu       & -Ofast -mtune=native -finline-limit=50000 &   \\
Intel     & -O3 -fast -fno-inline-factor -xHost       &   \\
Cray      & -O3 -O ipa5 -h wp                         &   \\
IBM       & -O5 -qinline=auto:level=10                &   \\
PGI       & {\bf Jeremy to supply}                    &   \\
\hline
\end{tabular}
\end{table}


Before applying any code transformations, we first benchmark the
original, serial version of the code. We also benchmark the vanilla,
unoptimised version after it has been re-structured following the
\psykal\ approach. In addition to this benchmarking, we profile these
versions of the code at the algorithm level (using a high-level timing
API). The resulting profiles are given in
Table~\ref{TABLE_profile}. The Momentum section dominates the profile
of both versions of the code, accounting for around 70--80\% of the
wall-clock time spent doing time-stepping. It is also the key section
when we consider the performance loss when moving from the original to
the \psykal\ version of the code; it slows down by a factor of
two. Although less significant in terms of absolute time, the
Continuity section is also dramatically slower in the \psykal\ version,
this time by a factor of three. In contrast, the performance of the
Time-update and Boundary Condition regions are not significantly
affected by the move to the \psykal\ version.

\begin{table}[!t]
\caption{The performance profile of the original and \psykal\ versions
  of NEMOLite2D on the Intel Ivy Bridge CPU (for 2000 time-steps of the
  $128^{2}$ domain and the Intel compiler).}
\label{TABLE_profile}
\centering
\begin{tabular}{l|c|c|c|c|c|c}
\hline
Section & \multicolumn{2}{c|}{Original} & \multicolumn{2}{c|}{Vanilla \psykal} &
\multicolumn{2}{c}{Final \psykal} \\
        & Time (s) & \%  & Time (s) & \%  & Time(s) & \% \\
\hline
Momentum    & 1.98  & 72.6  & 4.05  & 79.5 & 2.09 & 75.3 \\
Time-update & 0.40  & 14.6  & 0.41  & 8.1  & 0.29 & 10.6 \\
BCs         & 0.25  & 9.1   & 0.29  & 5.7  & 0.27 & 9.9  \\
Continuity  & 0.10  & 3.7   & 0.33  & 6.6  & 0.11 & 4.1  \\
\hline
\end{tabular}
\end{table}

Beginning with the vanilla \psykal\ version, we then apply a series of
code transformations while obeying the \psykal\ separation of concerns,
{\it i.e.} optimisation is restricted to the middle, {PS}y layer and
leaves the kernel and algorithm layers unchanged. The aim of these
optimisations is to recover, as much as is possible, the performance
of the original version of the code. The transformations we have
performed and the reasons for them are described in the following sections.

\subsubsection{Addition of {\tt safe\_address} directives}
\label{sec_safe_address}

Much of the optimisations we have performed have been informed by the
diagnostic output produced by either the Cray or Intel compilers. Many
of the NEMOLite2D kernels contain conditional statements. These
statements are there to check whether {\it e.g.} the current grid
point is wet or neighbours a boundary point. A compiler is better able
to optimise such a loop if it can be sure that all array accesses
within the body of the loop are safe for every trip, irrespective of
the conditional statements. In its diagnostic output the Cray compiler
notes this with messages of the form:
\begin{verbatim}
ftn-6375 ftn: VECTOR File = nemolite2d.f90,
 Line = 448 
A loop starting at line 448 would benefit 
from "!dir$ safe_address".
\end{verbatim}
In order that we could safely add such directives we altered the field
module to allocate all field-data arrays with extents greater than
strictly required.  We were then able to safely add the {\tt safe\_address}
before all of the loops where the Cray compiler indicated it might be
useful (the Momentum loops and some of the BC loops).

\subsubsection{In-line Momentum kernel bodies into middle-layer code}
\label{sec_inline_mom}

The profiling data in Table~\ref{TABLE_profile} shows that it is the
Momentum section that accounts for the bulk of the model run-time.  We
therefore chose to attempt to optimise this section first. In-keeping
with the \psykal\ approach, we are only permitted to optimise the middle
(PSy) layer which, for this section comprises calls to two kernels,
one for each of the $x$ and $y$ components of momentum. These kernels
are relatively large; each comprises roughly 85 lines of Fortran
(excluding variable declarations).

From our previous work~\cite{shallow_psykal} on a similar code we know
that kernel in-lining is critical to obtaining performance with both
the Gnu and Intel compilers. For the Gnu compiler, this is because it
cannot perform in-lining when routines are in separate source
files. In our previous work we obtained an order-of-magnitude speed-up
simply by moving subroutines into the module containing the middle
layer (from which the kernels are called). A further performance
improvement of roughly 30\% was obtained when the kernel code was
manually inserted at the site of the subroutine call.

Although the Intel compiler does do in-lining when routines are in
separate source files, we have found (both here and in our previous
work~\cite{shallow_psykal}) that the number of optimisations it
performs is reduced if it first has to in-line a routine. For the
Intel-compiled Shallow code, manually inserting kernel code at the
site of the subroutine call increased performance by about 25\%.

In fact, in-lining can have a significant effect on the Intel
compiler's ability to vectorise a loop. Taking the loop that calls the
kernel for the u-component of momentum as an example, before in-lining
the compiler reports:
\begin{verbatim}
LOOP BEGIN at time_step_mod.f90(85,7) 
  inlined into nemolite2d.f90(86,11)
 remark #15335: loop was not vectorized: 
  vectorization possible but seems 
  inefficient. 
 --- begin vector loop cost summary ---
 scalar loop cost: 1307 
 vector loop cost: 2391.000 
 estimated potential speedup: 0.540 
 lightweight vector operations: 1133 
 medium-overhead vector operations: 1 
 vectorized math library calls: 1 
 --- end vector loop cost summary ---
LOOP END
\end{verbatim}
After we have manually in-lined the kernel body, the compiler reports:
\begin{verbatim}
LOOP BEGIN at time_step_mod.f90(97,7) 
  inlined into nemolite2d.f90(86,11)
 LOOP WAS VECTORIZED
 --- begin vector loop cost summary ---
 scalar loop cost: 1253 
 vector loop cost: 521.750 
 estimated potential speedup: 2.350 
 lightweight vector operations: 697 
 medium-overhead vector operations: 1 
 vectorized math library calls: 1 
 --- end vector loop cost summary ---
LOOP END
\end{verbatim}
Looking at the `estimated potential speedup' in the compiler output
above, it is clear that the way in which the compiler vectorises the
two versions must be very different. This conclusion is borne out by
the fact that if one persuades the compiler to vectorise the first
version (through the use of a directive) then the performance of the
resulting binary is worse than that where the loop is left
un-vectorised. In principle this could be investigated further by
looking at the assembler that the Intel compiler generates but that is
outside the scope of this work.

For the best possible performance, we have therefore chosen to do full, manual
inlining for the two kernels making up the Momentum section.

\subsubsection{Force SIMD vectorisation of the Momentum kernels 
 using directives}

It turns out that the Cray-compiled binaries of both the original and
\psykal\ versions of NEMOLite2D perform considerably less well than
their Intel-compiled counterparts. Comparison of the diagnostic output
from each of the compilers revealed that while the Intel compiler was
happy to vectorise the Momentum loops, the Cray compiler was choosing
not to:
\begin{verbatim}
   99.          do ji = 2, M-1, 1
 A loop starting at line 99 was blocked 
 with block size 256.

 A loop starting at line 99 was not
 vectorized because it contains 
 conditional code which is more
 efficient if executed in scalar mode.
\end{verbatim}
Inserting the Cray {\tt vector always} directive persuaded the compiler
to vectorise the loop:
\begin{verbatim}
  99.    !dir$ vector always
  100.          do ji = 2, M-1, 1
 A loop starting at line 100 was blocked
 with block size 256.

 A loop starting at line 100 requires an 
 estimated 17 vector registers at 
 line 151; 1 of these have been 
 preemptively forced to memory.

 A loop starting at line 100 was vectorized.
\end{verbatim}
which gave a significant performance improvement. This behaviour is in
contrast to that obtained with the Intel compiler: its predictions
about whether vectorising a loop would be beneficial were generally
found to be reliable.

\subsubsection{Work around limitations related to derived types}

Having optimised the Momentum section as much as permitted by the
\psykal\ approach, we turn our attention to the three remaining
sections of the code. The profile data in Table~\ref{TABLE_profile}
shows that these regions are all comparable in terms of cost. What is
striking however, is that the cost of the Continuity section increases
by more than a factor of three in moving to the \psykal\ version of the
code.

Comparison of the diagnostic output from the Cray and Intel
compilers revealed that the Cray compiler was vectorising the
Continuity section while the Intel compiler reported that it was
unable to do so due to dependencies. After some experimentation we
found that this was due to limitations in the compiler's analysis of
the way components of Fortran derived types were being used. Each
GOcean field object, in addition to the array holding the local
section of the field, contains a pointer to a GOcean grid object. If a
kernel requires grid-related quantities ({\it e.g.} the grid spacing)
then these are obtained by passing it a reference to the appropriate
array within the grid object. Although these grid-related quantities
are read-only within a compute kernel, if they were referenced from the
same field object as that containing an array to which the kernel writes
then the Intel compiler identified a dependency preventing
vectorisation.  This limitation was simply removed by ensuring that
all read-only quantities were accessed via field objects that were
themselves read-only for the kernel at hand.  For instance, the call
to the continuity kernel, which confused the Intel compiler, originally
looked like this:
\begin{verbatim}
do jj = 2, N, 1
 do ji = 2, M, 1

   call continuity_code(ji, jj,      &
                        ssha%data,   &
                        sshn_t%data, &
                        ...,         &
                        ssha%grid%area_t)
\end{verbatim}
where {\it ssha} is the only field that is written to by the kernel.
We remove any potential confusion by instead obtaining the
grid-related (read-only) quantities from a field ({\it sshn\_t} in
this case) that is only read by the kernel:
\begin{verbatim}
do jj = 2, N, 1
 do ji = 2, M, 1

   call continuity_code(ji, jj,      &
                        ssha%data,   &
                        sshn_t%data, &
                        ...,         &
                        sshn_t%grid%area_t)
\end{verbatim}

\subsubsection{In-line the Continuity kernel}
\label{sec_cont_inline}

As with the Momentum kernel, we know that obtaining optimal
performance from both the Gnu and Intel compilers requires that a
kernel be manually in-lined at its call site. We do this for the
Continuity kernel in this optimisation step.

\subsubsection{In-line remaining kernels (BCs and time-update)}
\label{sec_inline_all_kernels}

Having optimised the Continuity section we finally turn our attention
to the Boundary Condition and Time-update sections. The kernels in
these sections are small and dominated by conditional statements.  We
therefore limited our optimisation of them to manually in-lining each
of the kernels into the PSy layer.

\subsubsection{In-line field-copy operations}
\label{sec_inline_fldcopy}

The Time-update section includes several array copies where fields for
the current time-step become the fields at the previous time-step.
Initially we implemented these copies as 'built-in' kernels (in the
GOcean library) as they are specified in the algorithm layer. However,
we obtained better performance by simply in-lining these array copies
into the PSy layer.

We shall see that the transformations we have just described do not
always result in improved performance. Whether or not they do so
depends both on the compiler used and the problem size. We also
emphasise that the aim of these optimisations is to make the PSy layer
as compiler-friendly as possible, following the lessons learned from
our previous work with the Shallow code~\cite{shallow_psykal}. It may
well be that transforming the code into some other structure would
result in better performance on a particular architecture. However,
exploring this optimisation space is beyond the scope of the present
work.

We explore the extent to which performance depends upon the problem
size by using square domains of dimension 64, 128, 256, 512 and 1024
for the traditional, cache-based CPU systems. This range allows us to
investigate what happens when cache is exhausted as well as giving us
some insight into the decisions that different compilers make when
optimising the code.

\subsection{Construction of OpenMP-Parallel NEMOLite2D}
\label{sec_omp_steps}

For this part of the work we began with the optimal \psykal\ version of
the code, as obtained after applying the various transformations
described in the previous section. As with the transformations of the
serial code, our purpose here is to determine the functionality
required of a tool that seeks to generate the PSy layer.

\subsubsection{Separate PARALLEL DOs}
\label{sec_sep_dos}

The simplest possible OpenMP-parallel implementation consists of
parallelising each loop nest in the PSy layer. This was done by
inserting an OpenMP PARALLEL DO directive before each loop nest so
that the iterations of the outermost or $j$ loop (over the latitude
dimension of the model domain) are shared out amongst the OpenMP
threads. This leaves the innermost ($i$) loop available for SIMD
vectorisation by the compiler.

The loop nest dealing with the application of the Flather boundary
condition to the y-component of velocity has a loop-carried dependency
in $j$ which appears to prevent its being executed in
parallel.\footnote{Only once this work was complete did we establish
  that boundary conditions are enforced such that it can be executed
  in parallel.}  This was therefore left unchanged and executed on
thread 0 only.

\subsubsection{Single PARALLEL region}
\label{sec_single_parallel}

Although very simple to implement, the use of separate PARALLEL DO
directives results in a lot of thread synchronisation and can also
cause the team of OpenMP threads to be repeatedly created and
destroyed. This may be avoided by keeping the thread team in existence
for as long as possible using an OpenMP PARALLEL region.  We therefore
enclosed the whole of the PSy layer (in this code, a single
subroutine) within a single PARALLEL region. The directive preceeding
each loop nest to be parallelised was then changed to an OpenMP DO. We
ensured that the v-Flather loop nest was executed in serial (by the
first thread to encounter it) by enclosing it within an OpenMP SINGLE
section.

\subsubsection{First-touch policy}
\label{sec_1sttouch}

When executing an OpenMP-parallel program on a Non-Uniform Memory
Access (NUMA) compute node it becomes important to ensure that the
memory locations accessed by each thread are local to the hardware
core upon which it is executing. One way of doing this is to implement
a so-called 'first-touch policy' whereby memory addresses that will
generally be accessed by a given thread during program execution are
first initialised by that thread. This is simply achieved by using an
OpenMP-parallel loop to initialise newly-allocated arrays to some
value, {\it e.g.} zero.

Since data arrays are managed within the GOcean Library this
optimisation can again be implemented without changing user code ({\it
  i.e.} the Application and Kernel layers).

\subsubsection{Minimise thread synchronisation}
\label{sec_synch}

By default, the OpenMP END DO directive includes an implicit barrier,
thus causing all threads to wait until the slowest has completed the
preceeding loop. Such synchronisation limits performance at larger
thread counts and, for the NEMOLite2D code, is frequently unecessary.
{\it E.g.} if a kernel does not make use of the results of a
preceeding kernel call then there is clearly no need for threads to
wait between the two kernels.

We analysed the inter-dependencies of each of the code sections within
the PSy layer and removed all unecessary barriers by adding the NOWAIT
qualifier to the relevant OpenMP END DO or END SINGLE directives. This
reduced the number of barriers from eleven down to four.

\subsubsection{Amortize serial region}
\label{sec_amortize}

As previously mentioned, the v-Flather section has to be executed in
serial becuase of a loop-carried dependence in $j$. (In principle we
could choose to parallelise the inner, $i$ loop but that would inhibit
its SIMD vectorisation.) This introduces a load-imbalance between the
threads. We attempt to mitigate this by moving this serial section to
before the (parallel) u-Flather section. Since these two sections are
independent, the aim is that the thread that performs the serial,
v-Flather computation then performs a smaller share of the following
u-Flather loop. In practice, this requires that some form of dynamic
thread scheduling is used.

\subsubsection{Thread scheduling}
\label{sec_scheduling}

In order to investigate how thread scheduling affects performance we
used the `runtime' argument to the OpenMP SCHEDULE qualifier for all
of our OpenMP parallel loops. The actual schedule to use can then be
set at run-time using the OMP\_SCHEDULE environment variable. We
experimented with using the standard static, dynamic and guided (with varying
chunk size) OpenMP schedules.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

We first consider the performance of the code in serial and examine
the effects of the transformations described in
Section~\ref{sec_methodology}. Once we have arrived at an optimal form
for the serial version of NEMOLite2D we then investigate its parallel
performance on both CPU- and GPU-based systems.

\subsection{Serial Performance}

In Figure~\ref{FIG_orig_perf_summary} we plot the serial performance
of the original version of the NEMOLite2D code for the range of
compilers and CPUs considered here. Unlike the Shallow
code~\cite{shallow_psykal}, the original version of NEMOLite2D has not
been optimised. Although it is still a single source file it is, in
common with NEMO itself, logically structured with separate
subroutines performing different parts of the physics within each time
step. This structuring and the heavy use of conditional statements
favour the Intel compiler which significantly out-performs both the
Gnu and Cray compilers. Only when the problem size falls out of cache
does the performance gap begin to narrow. The reason for the
performance deficit of the Cray-compiled binary comes down to (a lack
of) SIMD vectorisation, an issue that we explore below.

\begin{figure}[!t]
\centering
\includegraphics[width=2.8in]{orig_summary}
\caption{Summary of the performance of the original version of the 
NEMOLite2D code on 
the range of compilers and CPU-based systems under consideration.}
\label{FIG_orig_perf_summary}
\end{figure}

Moving now the to the \psykal\ version of NEMOLite2D,
Figure~\ref{FIG_best_psykal_perf_summary} plots the performance of the
fastest \psykal\ version for each of the compiler/CPU combinations.
While the Intel compiler still produces the best-performing binary,
the Cray-compiled binary is now a very close second. In fact, the
performance of both the Gnu- and Cray-compiled \psykal\ versions is
generally significantly greater than that of their respective original
versions. We also note that best absolute performance (in terms of
grid points processed per second) with any compiler is obtained with
the $256^2$ domain.

\begin{figure}[!t]
\centering
\includegraphics[width=7.5cm]{best_psykal_summary}
\caption{Summary of the best performance achieved by any \psykal\ 
version of NEMOLite2D with each of the compilers and CPUs under 
consideration.}
\label{FIG_best_psykal_perf_summary}
\end{figure}

Figure~\ref{FIG_slowdown_summary} plots the percentage difference
between the performance of the original and the best \psykal\ versions
of NEMOLite2D for each compiler/CPU combination. This shows that it is
only the Intel-compiled binary running the $64^2$ domain that is
slower with the \psykal\ version of the code (and then only by some
3\%). For all other points in the space, the optimised \psykal\ version
of the code performs better. The results for the Cray compiler are
however somewhat sqewed by the fact that it did not SIMD vectorise key
parts of the original version (see below).

\begin{figure}[!t]
\centering
\includegraphics[width=2.8in]{slowdown_summary}
\caption{Comparison of the performance of the best \psykal\
version with that of the original version of the code. A negative value 
indicates that the \psykal\ version is slower than the original.}
\label{FIG_slowdown_summary}
\end{figure}

Having show that we can recover, and often improve upon, the
performance of the original version of NEMOLite2D, the next logical
step is to examine the necessary code transformations in detail.  We
do this for the $256^{2}$ case since this fits within cache on the Ivy
Bridge CPUs we are using here.  Table~\ref{TABLE_opt_breakdown} shows
detailed performance figures for this case after each transformation has been
applied to the code. The same data is visualised in
Figure~\ref{FIG_opt_stages_256} where the performance of the \psykal\
version for a given compiler/CPU is given relative to the performance
of the original with the same compiler/CPU.

Looking at the results for the Gnu compiler (and the Ivy Bridge CPU)
first, all of the steps-up in performance correspond to kernel
in-lining. None of the other transformations had any effect on the
performance of the compiled code. In fact, simply in-lining the two
kernels associated with the Momentum section was enough to improve
upon the performance of the original code. 

With the Intel compiler, the single largest performance increase is
again due to kernel in-lining (of the Momentum kernels). This is
because the compiler does a much better job of SIMD vectorising the
loops involved than it does when it first has to in-line the kernel
itself (as evidenced by its own diagnostic output - see
Section~\ref{sec_inline_mom}). However, although this gives a
significant performance increase it is not sufficient to match the
performance of the original version. This is only achieved by
in-lining every kernel and making the lack of data dependencies
between arrays accessed from different field objects more explicit.

The Cray compiler is distinct from the other two in that kernel
in-lining does not give any performance benefit and in fact, for the
smaller kernels, it can actually hurt performance. Thus the key
transformation is to encourage the compiler to SIMD vectorise the
Momentum section via a compiler-specific directive (without this it
concludes that such vectorisation would be inefficient). Of the other
transformations, only the addition of the compiler-specific
safe\_address directive (see Section~\ref{sec_safe_address}) was found
to give any benefit.

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
\caption{Performance (millions of points updated per second) for the
  $256^2$ case after each code transformation. Where an optimisation
  utilises compiler-specific directives then performance figures for
  the other compilers are omitted.}
\label{TABLE_opt_breakdown}
\centering
\begin{tabular}{l|c|c|c}
\hline
Compiler:           & Gnu & Intel & Cray  \\
\hline
CPU:                & \multicolumn{3}{c}{Ivy Bridge}  \\
\hline
Original         & 3.87 & 12.2 & 6.83 \\
Vanilla \psykal  & 3.07 & 6.55 & 6.73 \\
Safe-address     & --   & --   & 6.95 \\
In-line Momentum & 4.31 & 10.7 & 6.93 \\
SIMD Momentum    & --   & --   & 11.8 \\
\parbox{2.5cm}{\raggedright Grid data from read-only objects} & 4.31 & 11.3 & 11.8 \\
In-line Continuity         & 4.83 & 11.8 & 11.6 \\
In-line remaining kernels  & 5.89 & 12.0 & 11.5 \\
In-line field copies       & 5.92 & 12.5 & 11.4 \\
\hline
\parbox{2.5cm}{\raggedright \%-speed-up of best {\it c.f.} original} & 34.6 & 2.39 & 42.2 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=93mm]{opt_stages_256}
\caption{Performance of the \psykal\ version of NEMOLite2D for the
  $256^{2}$ domain at each stage of optimisation. The first (black)
  bar of each cluster gives the performance of the original version of
  NEMOLite2D for that compiler/CPU combination.}
\label{FIG_opt_stages_256}
\end{figure}

% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.

%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.

% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.


\subsection{Parallel Performance}

We now turn to transformations related to parallelisation of the
NEMOLite2D code; the introduction of OpenMP and OpenACC directives. In
keeping with the \psykal\ approach we do not modify either the
Algorithm- or Kernel-layer code.  Any code changes are restricted to
either the PSy (middle) layer or the underlying library that manages
{\it e.g.} the construction of field objects.

\subsubsection{OpenMP Version}

As with the serial optimisations, we consider the effect of each of
the OpenMP optimisation steps described in Section~\ref{sec_omp_steps}
for the $256^2$ domain. For this we principally use a single Intel,
Ivy Bridge socket which has 12 hardware cores and support for up to 24
threads with hyperthreading (i.e. two threads per core).
Figures~\ref{FIG_gnu_omp_256},~\ref{FIG_intel_omp_256}
and~\ref{FIG_cray_omp_256} show the performance of each of the
versions of the code on this system for the Gnu, Intel and Cray
compilers, respectively.

In order to quantify the scaling behaviour of the different versions
of NEMOLite2D with the different compilers/run-time environments, we
also plot the parallel efficiency in
Figures~\ref{FIG_gnu_omp_256},~\ref{FIG_intel_omp_256}
and~\ref{FIG_cray_omp_256} (dashed lines and open symbols). We define
parallel efficiency (\%), $E(n)$, on a number of threads, $n$, as:
\[
E(n) = 100  \frac{P(n)}{ n P(1)}
\]
where $P(m)$ is the performance of the code on $m$ threads. For a
perfect, linearly-scaling code, $E(n)$ will be 100\%.

Since the space to explore consists of three different compilers, six
different domain sizes, five stages of optimisation and six different
thread counts we can only consider a slice through it in what follows.
In order to inform our choice of domain size,
Figure~\ref{FIG_omp_domain_size} shows the scaling behaviour of the
most performant, Intel-compiled version of NEMOLite2D. Although it is
common for production runs of NEMO to use MPI sub-domains of $20
\times 20$ ({\bf do we have/need a reference for that?}), it is clear
from Figure~\ref{FIG_omp_domain_size} that the limited quantity of
parallelism in the $32 \times 32$ domain inhibits scaling. Therefore,
to fully test the performance of our OpenMP implementations we have
chosen to examine results for the $256 \times 256$ domain and these
are shown in Figures~\ref{FIG_gnu_omp_256}, \ref{FIG_intel_omp_256}
and \ref{FIG_cray_omp_256}.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_problem_size}
\caption{The scaling behaviour of the most performant OpenMP-parallel
  version of \psykal\ NEMOLite2D for the full range of domain sizes
  considered on the CPU. Results are for the Intel compiler on a
  single Intel Ivy Bridge socket. The corresponding parallel
  efficiencies are shown using open symbols and dashed lines. The
  24-thread runs employed hyperthreading.}
\label{FIG_omp_domain_size}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_256_gnu}
\caption{Performance of the OpenMP-parallel version of \psykal\
  NEMOLite2D for the Gnu compiler on a single Intel Ivy Bridge
  socket. The corresponding parallel efficiencies are shown using open
  symbols and dashed lines. The 24-thread runs employed hyperthreading
  and the optimal OpenMP schedule is given in parentheses.}
\label{FIG_gnu_omp_256}
\end{figure}

The simplest OpenMP implementation (black lines, circle symbols) fails
to scale well for any of the compilers. For the Intel and Gnu
versions, parallel efficiency is already less than 50\% on just four
threads. The Cray version however does better and is about 45\%
efficient on eight threads.

With the move to a single PARALLEL region, the situation
is greatly improved with all three executables now scaling out to at
least 12 threads with $\sim70$\% efficiency (red lines, square symbols).

In restricting ourselves to a single socket, we are keeping all
threads within a single NUMA region. It is therefore surprising that
implementing a `first-touch' policy has any effect and yet, for the
Gnu- and Intel-compiled binaries, it appears to improve performance
when hyperthreading is employed to run on 24 threads (green lines and
diamond symbols in Figures~\ref{FIG_gnu_omp_256} and
~\ref{FIG_intel_omp_256}).

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_256_intel}
\caption{Performance of the OpenMP-parallel version of \psykal\
  NEMOLite2D for the Intel compiler on a single Intel Ivy Bridge
  socket.  The corresponding parallel efficiencies are shown using
  open symbols and dashed lines. The 24-thread runs employed
  hyperthreading and the optimal OpenMP schedule is given in
  parentheses.}
\label{FIG_intel_omp_256}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\picwidth]{omp_scaling_256_cray}
\caption{Performance of the OpenMP-parallel version of \psykal\
  NEMOLite2D for the Cray compiler on a single Intel Ivy Bridge
  socket.  The corresponding parallel efficiencies are shown using
  open symbols and dashed lines. The 24-thread runs employed
  hyperthreading and the optimal OpenMP schedule is given in
  parentheses.}
\label{FIG_cray_omp_256}
\end{figure}

The final optimisation step that we found to have any significant
effect is to minimise the amount of thread synchronisation by
introducing the NOWAIT qualifier wherever possible (blue lines and
upwards-triangle symbols). For the Gnu compiler, this improves the
performance of the executable on eight or more threads while, for the
Intel compiler, it only gives an improvement for the 24-thread
case. Moving the SINGLE region before a parallel loop is marginally
beneficial for the Gnu- and Cray-compiled binaries and yet reduces the
performance of the Intel binary (purple lines and right-pointing
triangles).

We have used different scales for the $y$-axes in each of the plots in
Figures~\ref{FIG_gnu_omp_256},~\ref{FIG_intel_omp_256}
and~\ref{FIG_cray_omp_256} in order to highlight the performance
differences between the code versions with a given compiler. However,
the best performance obtained on twelve threads ({\it i.e.}  without
hyperthreading) is 52.1, 55.6 and 46.3 (million points/s) for the Gnu,
Intel and Cray compilers, respectively. This serves to emphasise the
democratisation that the introduction of OpenMP has had; for the
serial case the Cray- and Intel-compiled exectables were a factor of
two faster than the Gnu-compiled binary
(Figure~\ref{FIG_opt_stages_256}). In the OpenMP version, the
Gnu-compiled binary is only 6\% slower than that produced by the Intel
compiler and is 13\% {\em faster} that that of the Cray compiler. In
part this situation comes about because of the effect that adding the
OpenMP compiler flag has on the optimisations performed by the
compiler.  In particular, the Cray compiler no-longer vectorises the
key Momentum section, despite the directive added during the serial
optimisation work.

Since NEMO and similar finite-difference codes tend to be
memory-bandwidth bound, we checked the sensitivity of our performance
results to this quantity by benchmarking using two sockets of Intel
Ivy Bridge ({\it i.e.} using a complete node of Archer). For this
configuration we ensured that threads were evenly shared over the two
sockets. The performance obtained for the $256^2$ case with the `early
SINGLE' version of the code is compared with the single-socket
performance in Figure~\ref{FIG_omp_2socks}. Surprisingly, doubling the
available memory bandwidth in this way has little effect on
performance - the two-socket performance figures track those from a
single socket very closely. The only significant difference in
performance is at 24 threads where, in addition to the difference in
available memory bandwidth, the single-socket configuration is using
hyperthreading while the two-socket case is not.  The discrepancy in
the performance of the Cray- and Intel-compiled binaries at this
thread count is under investigation by Cray.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{1and2sockets}
\caption{Performance of the OpenMP-parallel version of \psykal\
  NEMOLite2D on one and two sockets of Intel Ivy Bridge. The 24-thread
  runs on a single socket used hyperthreading and the two-socket runs
  had the threads shared equally between the sockets.}
\label{FIG_omp_2socks}
\end{figure}

A further complication is the choice of scheduling of the OpenMP
threads.  We have investigated the performance of each of the
executables (and thus the associated OpenMP run-time library) with the
standard OpenMP {\it static}, {\it dynamic} and {\it guided}
scheduling policies~\cite{openmp_standard}. For the Intel
compiler/run-time, static loop scheduling was found to be best for all
versions apart from that where we have attempted to amortize the cost
of the SINGLE section. This is to be expected since that strategy
requires some form of dynamic loop scheduling in order to reduce the
load imbalance introduced by the SINGLE section.

In contrast, some form of dynamic scheduling gave a performance
improvement with the Gnu compiler/run-time even for the `first-touch'
version of the code. This is despite the fact that this version
contains (implicit) thread synchronisation after every parallel loop.
For the Cray compiler/run-time, some form of dynamic scheduling became
optimal once inter-thread synchronisation was reduced using the NOWAIT
qualifiers.


%\begin{figure}
%\centering
%\includegraphics[width=\picwidth]{omp_scaling_32_gnu}
%\caption{Performance of the OpenMP-parallel version of \psykal\
%  nemolite2d for the Gnu compiler on a single Intel Ivy Bridge
%  socket. The corresponding parallel efficiencies are shown using open
%  symbols and dashed lines. The 24-thread runs employed hyperthreading
%  and the optimal OpenMP schedule is given in parentheses.}
%\label{FIG_gnu_omp_32}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=\picwidth]{omp_scaling_32_intel}
%\caption{Performance of the OpenMP-parallel version of \psykal\
%  nemolite2d for the Intel compiler on a single Intel Ivy Bridge
%  socket.  The corresponding parallel efficiencies are shown using
%  open symbols and dashed lines. The 24-thread runs employed
%  hyperthreading and the optimal OpenMP schedule is given in
%  parentheses.}
%\label{FIG_intel_omp_32}
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=\picwidth]{omp_scaling_32_cray}
%\caption{Performance of the OpenMP-parallel version of \psykal\
%  nemolite2d for the Cray compiler on a single Intel Ivy Bridge
%  socket.  The corresponding parallel efficiencies are shown using
%  open symbols and dashed lines. The 24-thread runs employed
%  hyperthreading and the optimal OpenMP schedule is given in
%  parentheses.}
%\label{FIG_cray_omp_32}
%\end{figure}

\subsubsection{OpenACC Version}

The introduction of OpenACC directives proved to be simpler than
producing the OpenMP version of NEMOLite2D {\bf - Jeremy to clarify
  whether this is true}.  Consequently we only show the performance of
{\bf xxx} versions of the code in Figure~\ref{FIG_gpu_opt_stages}.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{gpu_opt_stages}
\caption{Performance of the various GPU implementations of NEMOLite2D.}
\label{FIG_gpu_opt_stages}
\end{figure}

In Figure~\ref{FIG_cpu_cf_gpu} we compare the absolute performance of
the OpenMP and OpenACC implementations of NEMOLite2D across the range
of problem sizes considered. The OpenMP figures are the maximum
performance obtained from a whole Intel Ivy Bridge socket by any
version of the code on any number of threads for a given
compiler/run-time. For the smallest domain size ($64^2$) the OpenMP
version significantly outperforms the GPU because there is
insufficient parallelism to fully utilise the GPU and one time-step
takes only 80~$\mu$s. The execution of a single time-step is then
dominated by the time taken to launch a kernel on the GPU rather than
the execution of the kernel itself.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{cpu_cf_gpu}
\caption{Performance of the best OpenMP-parallel version of \psykal\
  NEMOLite2D (on a single Intel Ivy Bridge socket) compared with the best GPU implementation. {\bf This should really be best OpenACC version as that's what PSyclone can support.}}
\label{FIG_cpu_cf_gpu}
\end{figure}

Once the problem size is increased to $128^2$, a single time-step
takes roughly 200~$\mu$s and only the Intel-compiled OpenMP version is
comparable in performance to the OpenACC version. For all of the
larger problem sizes plotted in Figure~\ref{FIG_cpu_cf_gpu} the GPU
version is considerably faster than the CPU.  For problem sizes of
$1024^2$ and greater, the 30MB cache of the Ivy Bridge CPU is
exhausted and performance becomes limited by the bandwidth to main
memory. At this stage the OpenACC version of the code on the GPU is
some {\bf 3.7 (this is the factor for the CUDA version!)} times faster
than the OpenMP version on the CPU. {\bf Can we relate this to the
  difference in peak memory b/w between the GPU and CPU?}

\subsection{Performance Analysis with the Roofline Model}

Although we have investigated how the performance of the \psykal
version of NEMOLite2D compares with that or the original, we have not
addressed how efficient the original actually is. In order to do so we
use the Roofline Model~\cite{roofline} which provides a relatively
simple way of characterising the performance of a code in terms of
whether it is memory-bandwidth bound or compute bound. To do so we
follow the approach suggested in~\cite{para_pearls} and use the
STREAM~\cite{stream} and LINPACK~\cite{linpack} benchmarks in order to
get the upper bounds on the memory bandwidth and floating-point
operations per second (FLOPS) for the test hardware. Since we are
using an Intel Ivybridge CPU we used the Intel Math Kernel Library
implementation of LINPACK.

A key component of the Roofline model is the Arithmetic Intensity of
the code being executed:
\begin{equation}
AI = \frac{\textrm{No. of floating-point operations}}{\textrm{Bytes fetched from memory}}
\end{equation}
We calculated this quantity manually by examining the
source code and counting the number of memory references and
arithmetic operations that it contained. In doing this counting we
assume that any references to adjacent array elements are fetched in a
single cache-line and thus only count once.

\begin{figure}
\centering
\includegraphics[width=\picwidth]{roofline}
\caption{Comparison of the performance achieved by NEMOLite2D and
  Shallow on the E5-1620 CPU. Results for the former are for the
  $256^2$ domain since that gave the best performance.}
\label{FIG_roofline}
\end{figure}

In Figure~\ref{FIG_roofline} we show the performance of kernels from
both Shallow and NEMOLite2D on the roofline model for the E5-1620 CPU.
This demonstrates that the Shallow kernel is achieving a performance
roughly consistent with saturating the available memory bandwidth. In
contrast, the kernel taken from NEMOLite2D is struggling to reach a
performance consistent with saturating the bandwidth to main
memory. We experimented with reducing the problem size (so as to
ensure it fitted within cache) but that did not significantly improve
the performance of the kernel. This then points to more fundamental issues
with the way that the kernel is implemented which are not captured in the
simple roofline model.

In order to aid our understanding of kernel performance we have
developed a tool capable of parsing Fortran code and generating a
Directed Acyclic Graph (DAG) of the data flow. This tool eases the
laborious and error-prone process of counting memory accesses and
FLOPs as well as providing information on those operations that are
rate-limiting or on the critical path. Using the details of the Intel
Ivy Bridge microarchitecture published by Fog~\cite{fog_microarch,
  fog_instructions} we have constructed performance estimates of the
NEMOLite2D kernel. By ignoring all Instruction-Level Parallelism
(i.e. assuming that all nodes in the DAG are executed in serial) we
get a lower-bound performance estimate of $0.6391 \times CLOCK\_SPEED$
FLOPS which gives 2.46~GFLOPS at a clock speed of
3.85~GHz.

Alternatively, we may construct an upper bound by assuming the
out-of-order execution engine of the Ivy Bridge core is able to
perfectly schedule and pipeline all operations such that those that go
to different execution ports are always run in parallel. In the Ivy
Bridge core floating-point multiplication and division operations go
to port 0 while addition and subtraction go to port
1~\cite{fog_instructions}. Therefore we sum the cost of all
multiplication and division operations in the DAG and compare that
with the sum of all addition and subtraction operations. The greater
of these two quantities is then taken to be the cost of executing the
kernel; all of the operations on the other port are assumed to be done
in parallel. This gives a performance estimate of $1.029 \times
CLOCK\_SPEED$ FLOPS or 3.96 GFLOPS at 3.85~GHz.

These performance estimates are plotted as CPU ceilings (horizontal
dashed lines) in Figure~\ref{FIG_roofline}. The performance of the
Momentum kernel is seen to fall between these two bounds which
demonstrates that its performance is not memory-bandwidth limited, as
might have been assumed by its low AI. Therefore, although the
performance of this kernel is clearly sub-optimal, this is due to the
balance of floating-point operations that it contains. Improving this
is outside the scope of the current work since here we are focused on
the \psykal separation of concerns and the introduction of parallelism
in the PSy layer.

Enabling SIMD vectorisation for this kernel does not significantly
improve its performance (Figure~\ref{FIG_roofline}) and in fact,
limiting it to SSE instructions (vector width of two double-precision
floating point numbers) rather than AVX (vector width of four) was
found to produce a slightly more performant version. The poor
efficiency of the SIMD version is highlighted by the fact that the
corresponding kernel performance no longer falls within the bounds of
our performance estimate. This is because we have incorporated the
effect of SSE by assuming perfect vectorisation which gives a
performance increase of a factor of two.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

We have investigated the application of the \psykal\ separation of
concerns approach to the domain of shared-memory parallel,
finite-difference ocean models. This approach enables the
computational science (performance) related aspects of a computer
model to be kept separate from the natural (oceanographic) science
aspects.

We have used a new and un-optimised, two-dimension model extracted
from the NEMO ocean model for this work. As a consequence of this, the
introduction of the \psykal\ separation of concerns followed by
suitable transformations of the PSy Layer is found to improve
performance. This is in contrast to our previous
experience~\cite{shallow_psykal} with tackling the Shallow code which
has had been optimised over the course of several years. In that case
we were able to recover\ldots

The application of code transformations to the middle/PSy layer is key
to the performance of the \psykal\ version of a code.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% use section* for acknowledgement
\section*{Acknowledgments}

This work made use of the ARCHER UK National Supercomputing Service
(\url{http://www.archer.ac.uk}) and some {\bf NVIDIA system - or is it
  Emerald?}.

% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,nemolite2d_perf}
%

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiographynophoto}{A.~R.~Porter}
Biography text here.
\end{IEEEbiographynophoto}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{R.~Ford}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{G.~Riley}
Biography text here.
\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\end{document}



The authors thank the reviewers for their detailed feedback on our paper.
We address the points raised below, beginning with the first (anonymous)
referee and then moving on to those from Carlos Osuna.

Anonymous referee:

1/ The reviewer is right to be concerned with seeing how much
   performance may be lost by the re-structuring required for
   PSyKAl. However, the original form of the NEMOLite2D code that we
   started with for this work was unoptimised. This is emphasised by
   the results in Figure 4 which show that, for the majority of cases,
   the final PSyKAl version was significantly faster. In contrast, our
   previous paper (Porter et al, 2016) tackled the "Shallow" code
   which has had optimisations applied to it over a period of some
   twenty years and was therefore a much more demanding test. There,
   the initial PSyKAl re-structuring did significantly harm
   performance. However, we showed that it was possible to recover the
   performance of the optimised original by applying code
   transformations that were PSyKAl-compliant. We therefore do not
   think that showing results for the optimisations
   applied to the original form of NEMOLite2D is necessary.

2/ The PSyKAl approach is in fact applicable to both Finite Element and
   Finite Difference codes (and in fact, LFRic, the largest project making use
   of it is using Finite Elements). We have updated the title to make
   this clear.

3/ A new section, "Related Approaches", has been added to the paper. In this
   we bring together a comparison of various other approaches including OCCA.
   This makes explicit the differences and makes clearer the advantages of
   PSyKAl. 

4/ We also use the new "Related Approaches" section to emphasise that
    although the tool to generate the PSy layer exists ("PSyclone"),
    the current paper is about code structuring.

Carlos Osuna's general review:

We have added a few key lines of source code from the Momentum kernel
in section 3.3. We have also added text that explains that the
apparently poor performance of this kernel is down to the number of
division operations.  On the Ivy Bridge, division operations require
at least eight times as many cycles as a multiplication which can be
very significant.

The suggestion of a "Related Work" section is a good one and we have added
this just after the Introduction.

We attribute the better performance of the SSE version of the code to the
fact that the poor SIMD efficiency of the code is failing to amortize the
larger cost of the peel and remainder loops associated with the greater
vector width of AVX. We have expanded upon this in the text.

Carlos Osuna's specific comments:

(Top of page 8.) We have clarified why allocating larger arrays was
necessary in order to use the safe_address directive.

Changed "Intel does..." to "Intel can..." as requested.

Sec 2.1.5 - the grid is not attached to the field, rather each field
keeps a pointer back to the grid upon which it is defined. This approach
was chosen in order to support applications which use more than one grid.

Sec 2.1.8 - it is not clear why in-lining manually helps (especially since
one would expect the Intel compiler in particular to do this). We suspect
that the change in the PSy-layer code causes the compiler to make some
different optimisation decisions and these happen to be beneficial. A
proper investigation of this is outside the scope of this paper. We have
added a sentence about this.

Sec 2.2.1 - yes, other strategies might be beneficial, especially blocking.
The key point is that adopting the PSyKAl structure does not limit these
possibilities.

Figure 3 - The Gnu results are much slower because gfortran is only SIMD
vectorising the loop containing the Continuity kernel. For some reason
it does not vectorise any others. Unlike Cray, there is no directive
that can be inserted to mandate that a loop be SIMD vectorised. We have
added text to this effect.


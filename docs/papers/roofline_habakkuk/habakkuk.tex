\documentclass[12pt]{article}
\usepackage[pdftex]{graphicx}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage{url}

\newcommand{\psykal}{{PS}y{KA}l}

\begin{document}

\title{Analysing the Performance of Shallow-Water Models with the
 Roofline Model}

\author{A.~.R.~Porter and R.~W.~Ford}

\maketitle

\tableofcontents

\section{Introduction}

The Roofline Model (RM)~\cite{roofline} has become a very popular way
of examining the performance of a given computational kernel and
contrasting it with the various limits associated with the system upon
which it is executing. However, its simplicity naturally means that
it has limitations.

In this work we examine those limitations in light of two different
shallow-water, finite-difference models. We explore the reasons behind
the limited utility of the RM for these two models and present a
tool, Habakkuk, that uses static code analysis to provide more
realistic performance predictions for Fortran kernels.

\begin{itemize}
\item Shallow implementation of Sadourny~\cite{sadourny75}.
\item NEMOLite2D.
\item NEMO
\end{itemize}

\section{The Roofline Model and its Limitations}

\subsection{What the Roofline Model (RFM) is}

The construction of a Roofline Model requires the peak floating-point
operations per second (FLOPS) and memory bandwidth (GB/s) that the
computer hardware is capable of. Although theoretical values for these
quantities may be computed (using the technical specification of
the machine, e.g. the number of floating-point instructions that may
be issued per clock cycle).

information on both the
computer being used and the code being run. For the former,

Arithmetic intensity and MAX(memory bandwidth, FLOPS)

A key component of the Roofline Model is the Arithmetic Intensity (AI)
of the code being executed:
\begin{equation}
AI = \frac{\textrm{No. of floating-point operations}}{\textrm{Bytes fetched from memory}}
\end{equation}

\subsection{Approaches to constructing a RFM}

Theoretical limits versus experimentally-measured values (STREAM and
LINPACK).

Measuring Arithmetic Intensity. Hardware Performance Counters (access,
reliability, difficulty of interpretation) or eye-balling the code
(error prone and time consuming).

\subsection{Basic Assumptions}
\subsection{When the assumptions break down}

All FLOPs are not equal. For instance, on Ivy Bridge a division costs
approximately eight times as much as a multiplication.
Instruction-level parallelism - operations despatched to different
execution ports can be executed in parallel.

\section{How to remove some of the assumptions}

Parse the (Fortran) source code.  Construct a Directed Acyclic Graph
to represent the source.  Use the properties of the DAG to
characterise the processed source.  At its simplest, this can just be
a count of FLOPs and memory (array) accesses.  Can then be extended to
allow for caching of consecutive array accesses as in stencil
operations in a direct-addressed model.

\subsubsection{Scheduling}

Obtaining a performance estimate by simply counting all of the FLOPs
that must be performed is equivalent to assuming that they are all
executed sequentially. However, a core on any recent generation of CPU
is likely to have support for Instruction Level Parallelism. For
instance, in Intel Ivybridge micro-architecture a core has six
execution ports to which micro-operations are
despatched. Floating-point multiplication and division is handled by
port 0 while addition is handled by port 1.

\begin{figure}
\includegraphics[width=130mm]{mulsd_divsd}
\caption{Cost in cycles on an Intel E5-1620 v.2 CPU of the inner-loop
  body as a function of number of independent MULSD and DIVSD
  instructions.}
\label{FIG_mul_div_overlap}
\end{figure}

\subsection{Single Instruction Multiple Data}

\subsection{Multi-core Performance}

So far we have considered only single-core performance. In reality,
any application seeking to achieve good performance will run in
parallel on multiple CPU cores.  If we restrict ourselves to running a
single process/thread per core then clearly that thread will have
exclusive use of that core's registers and execution units. Resources
shared between cores (such as L2 cache or bandwidth to main memory) will
however be shared by the executing threads.

\section{Building a Model of a CPU Core}

We require various parameters that characterise the performance of a
CPU core, the value of these are micro-architecture dependent.

\begin{itemize}
\item Vector width
\item Size of a cache line
\item Support for FMA
\item Support for out-of-order execution
\item Available Instruction-Level Parallelism
\item Cost of arithmetic operations
\end{itemize}

\subsection{Micro-Benchmarks}

\begin{itemize}
\item Fortran for high-level, intrinsic ops (e.g. SIN())
\item Likwid for low-level, individual arithmetic ops (e.g. floating-point division)
\item Results from Agner Fog
\end{itemize}

\section{Results}

\subsection{Shallow}

\subsection{NEMOLite2D}

Although the Continuity kernel is very simple, the u-Momentum kernel
is not and in particular, includes a call to the {\tt sin()} function.
In order to estimate the cost of this function in FLOPs, the following
code was compiled (with the Intel compiler and -O1) and run with
likwid-perfctr:
\begin{verbatim}
  my_sum = 0.0d0
  do i = 1, 1000
    my_sum = my_sum + sin(real(i))
  end do
  write (*,*) 'My sum = ', my_sum
\end{verbatim}
This measurement revealed that the {\tt sin()} function requires
approximately 13 FLOPs.

A key component of the Roofline model is the Arithmetic Intensity of
the code being executed:
\begin{equation}
AI = \frac{\textrm{No. of floating-point operations}}{\textrm{Bytes fetched from memory}}
\end{equation}

Since HWPC data proved to be unreliable, we calculated this quantity
manually by examining the source code and counting the number of
memory references and arithmetic operations that it contained. In
doing this counting we assume that any references to adjacent array
elements are fetched in a single cache-line and thus only count once.

To illustrate this, we consider the first loop-nest in the
time-stepping section of the Shallow code. This is a doubly-nested
loop over {\tt i} and {\tt j} with {\tt i} innermost. The body of the
loop is:
\begin{verbatim}
CU(I+1,J) = .5d0*(P(I+1,J)+P(I,J))*U(I+1,J)
CV(I,J+1) = .5d0*(P(I,J+1)+P(I,J))*V(I,J+1)
Z(I+1,J+1) =(FSDX*(V(I+1,J+1)-V(I,J+1))- &
             FSDY*(U(I+1,J+1)-U(I+1,J)))/&
            (P(I,J)+P(I+1,J)+            &
             P(I+1,J+1)+P(I,J+1))
H(I,J) = P(I,J)+0.25d0*                   &
         (U(I+1,J)*U(I+1,J)+U(I,J)*U(I,J) & 
         +V(I,J+1)*V(I,J+1)+V(I,J)*V(I,J))
\end{verbatim}
This code writes to four memory locations ({\tt CU}, {\tt CV}, {\tt Z}
and {\tt H}) and reads from ten distinct locations.  However, if we
assume caching of reads from locations adjacent in memory ({\it e.g.} {\tt
  P(i,j)} and {\tt P(i+1,j)}) then we have only six distinct reads.
If we assume that each of these ten memory accesses will touch a
separate cache line then this code fragment will result in the moving
of ten cache lines, each of which is 64 bytes (eight double-precision
words). However, as noted above, this code fragment is the body of a
loop nest. Therefore, upon the next loop trip, {\tt i} will have been
incremented by one and the majority of the accessed values will still
be in cache. In fact, a new cache line will only have to be fetched
every seven trips of the (innermost) loop and thus the mean memory
traffic per loop trip is approximately $640/7$ bytes.

We can also count the number of floating-point arithmetic operations
(FLOPs) in this code fragment which gives us 12 multiplications and 12
additions. The $AI$ for this code fragment is then $24*7/640 = 0.26$
FLOPs/byte.  If we do the same analysis for the (longer) u-Momentum
kernel of NEMOLite2D then we find that there are 51 addition
operations, 56 multiplication/division operations and one call to {\tt
  sin()}. There are 31 distinct memory references. This gives an $AI$
of approximately $(107+13)*7/(31*64) = 0.42$ FLOPs/byte (where we
assume the call to {\tt sin()} costs 13 FLOPs). The results of this
analysis are summarised in Table~\ref{TAB_kernel_details}.

\begin{table}
\begin{tabular}{|l|c|c|c|}
\hline
                             & Shallow & \multicolumn{2}{c|}{NEMOLite2D} \\
WSS of time-stepping (bytes) & $104n^2$ & \multicolumn{2}{c|}{$180n^2$} \\ 
\hline
Kernel                     & 1st loop nest & Continuity & u-Momentum \\
\hline                                
FLOPs                      & 24      &    14      &   107+13   \\
Distinct memory refs       & 10      &    12      &   31       \\
$AI$ (FLOPs/byte)          & 0.26    &  0.128     &   0.42     \\
WSS (bytes)                & $56n^2$ &  $72n^2$   &   $136n^2$ \\
\hline
\end{tabular}
\caption{Details of the kernels investigated with the roofline
  model. WSS is the Working Set Size and $n$ is the problem size --
  the length of one side of the square domain.}
\label{TAB_kernel_details}
\end{table}

We measure the mean time to execute the u-Momentum kernel over the
whole domain as 0.0021 seconds (Intel v.16 on the E5-1620) for the
$256^2$ problem size. This gives us a performance of $\frac{120\times
  256^2}{0.0021} = 3.7$ GFLOPS. (In contrast, instrumenting this
section of code with the LIKWID marker API and using likwid-perfctr we
measure 4.7 GFLOPS.)  For the $128^2$ domain, the measured time is
0.556E-03 s. This gives a performance of $\frac{120\times
  128^2}{0.556\times10^{-3}} = 3.5$ GFLOPS which again is some 25\% less
than we measure directly with likwid.

Using these figures we can finally construct the Roofline model for
the two CPUs and the various kernels. In
Figure~\ref{FIG_roofline_archer} we plot the model for the E5-2697 CPU
and show the performance of the 1st loop-nest of Shallow and the
u-Momentum kernel of NEMOLite2D. Two things are immediately clear from
this plot: first, the performance of the first loop nest of Shallow is
clearly memory-bandwidth limited and second, the u-Momentum kernel of
NEMOLite2D is no-where near approaching this limit.

\begin{figure}
\centering
\includegraphics[width=120mm]{roofline_archer}
\caption{Comparison of the performance achieved by NEMOLite2D and
  Shallow on the E5-2697 CPU.}
\label{FIG_roofline_archer}
\end{figure}

Two performance points are plotted for the Shallow kernel
corresponding to domains of $512^2$ and $1024^2$. Using the equation
for working-set size in Table~\ref{TAB_kernel_details}, these
correspond to 14.7 and 58.7~MB, respectively. Since the E5-2697 has an
L3 cache of 30~MB, the $512^2$ case should be contained in cache while
the $1024^2$ case will spill to main memory. This is borne out by the
plot; the $1024^2$ case is achieving a performance roughly in-line
with saturating the bandwidth to main memory. The $512^2$ case is
approaching the L3 bandwidth limit. We investigate the reasons for the
fact that it does not achieve the L3 bandwidth limit below.
 
In contrast to the results obtained with the Shallow kernel, the
performance of the NEMOLite2D u-Momentum kernel shows little
sensitivity to working-set size -- the $128^2$, $256^2$ and $512^2$
performance points in Figure~\ref{FIG_roofline_archer} are very close
together and well below any of the roofline limits.

In Figure~\ref{FIG_roofline_desktop} we plot the Roofline model
obtained for the E5-1620 CPU. In order to investigate the poor
performance of the u-Momentum kernel of NEMOLite2D, we also show the
performance of the (simpler) Continuity kernel from that code. As we
would expect, the performance of the u-Momentum kernel remains poor on
this CPU. For the $512^2$ domain the Continuity kernel performs well,
reaching the limit imposed by the bandwidth to main memory. However,
the working-set sizes for this kernel for the $128^2$ and $256^2$
domains are approximately 1.1~MB and 4.7~MB, respectively and
therefore both of these should fit comfortably within L3
cache. Despite this, the performance achieved with these domain sizes
falls short of the L3 bandwidth limit.  Even when we consider that the
Continuity kernel is just one of several within the time-stepping loop
of NEMOLite2D we see that the L3 bandwidth limit is still applicable;
the loop as a whole has a working-set size of $180\times n^{2}$
bytes. For the $32^2$ domain this is 180~KB ({\it i.e.}\ within L2),
while the $52^2$ (475~KB) through to the $128^2$ domain (2.9~MB) fall
well within the capacity of L3.

\begin{figure}
\centering
\includegraphics[width=120mm]{roofline_desktop}
\caption{Comparison of the performance achieved by different
  NEMOLite2D kernels on the E5-1620 CPU.}
\label{FIG_roofline_desktop}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=120mm]{roofline_desktop_cont_limits}
  \includegraphics[width=120mm]{roofline_desktop_mom_limits}
  \caption{Roofline with computed ceilings for the Continuity and
    Momentum kernels.
    Kernel performance shown
    is for the $256^2$ domain on the E5-1620 CPU.}
\end{figure}

In order to investigate why the performance of the Continuity kernel
is falling well short of the maximum predicted by the roofline model
we have experimented with the level of SIMD
vectorisation. Figure~\ref{FIG_cont_vec} shows the performance of the
Continuity kernel compiled with differing levels of SIMD vectorisation
and after code modifications intended to improve vectorisation.
Compiling without auto-vectorisation (compiler flag ``-no-vec''), the
kernel achieves a performance that appears to be largely limited by
the bandwidth to main memory. Note that there is very little
opportunity for temporal cache re-use in the Continuity kernel -- only
three of the twelve distinct memory references are to previous rows of
the array. The size of this effect can be seen in the small drop in
performance (black bars) in going from the $256^2$ to the $512^2$
domain as at this point the working-set size of this kernel spills out
of L3 cache.

\begin{figure}
  \centering
  \includegraphics[width=130mm]{continuity_opt}
  \caption{Performance of the Continuity kernel with optimisations
    intended to aid SIMD vectorisation. Results are for v.16 of the
    Intel compiler on a Xeon E5-1620 v.2 CPU. Optimisations are
    cumulative for results with a given SIMD instruction set.}
  \label{FIG_cont_vec}
\end{figure}

The E5-1620 CPU supports two different SIMD instruction sets;
Streaming SIMD Extensions (SSE; flag ``-axSSE4.2'') and Advanced
Vector Extensions (AVX; flag ``-xHost''). SSE supports vector lengths
of 128 bits (two double-precision words) and AVX 256 bits (four
double-precision words). For the problem sizes that largely fit within
L3 or L2 cache, switching on the use of SSE instructions provides a
speed-up of between 1.5 and 1.8 - the factor increasing with problem
size until the WSS of the time-stepping loop begins to fall out of L3
cache. It is therefore likely that the reason this factor is below the
ideal value of 2.0 is the overhead associated with constructing the
loop and handling any ``peel'' or ``remainder'' loops. As the problem
size increases, this overhead (as a fraction of the vectorisable
workload) decreases. {\bf if this is the case then why is my 128x8 domain
not faster than the 32x32?}

When compiling with AVX instructions rather than SSE, the performance
of the Continuity kernel was typically just below that of the latter
for all of the problem sizes fitting within cache. We attribute this
to the higher overheads associated with managing the longer AVX
vectors.

Examining the optimisation report produced by the Intel compiler
revealed that the majority of array accesses within the loop were not
aligned:
\begin{verbatim}
Example of unaligned access report here
\end{verbatim}
and that this inhibited efficient vectorisation. Tackling this issue
requires two steps; first, ensure that the arrays are allocated to
align with 64-byte boundaries and second, ensure that the compiler is
aware of this where those arrays are used. The first requirement is
easily met by passing the ``-align array64byte'' flag to
the compiler when building the code. This ensures that any array that
is constructed using a call to ALLOCATE is aligned on 64-byte
boundaries in memory.

The second requirement is complicated by the fact that the NEMOLite2D
code makes extensive use of Fortran derived types --- the data arrays
used for computation are members of derived types. The information
about the alignment of these arrays cannot therefore be propagated
through the code by module {\tt use} statements. In fact, even the
Intel compiler directive which can be used to specify the alignment
of a particular array at its point of use will not accept references
to members of a derived type. It was therefore necessary to wrap the
kernel call (and associated loops) within a routine and pass in
references to the arrays. Within the wrapper routine the corresponding
dummy arguments are declared as simple arrays and their alignment can
then be specified using the {\tt ASSUME\_ALIGNED} directive:
\begin{verbatim}
subroutine invoke_continuity_arrays(nx, ny, ssha, ...)
  integer, intent(in)   :: nx, ny
  real(wp), intent(out) :: ssha(nx,ny)
!DIR$ ASSUME_ALIGNED ssha:64
  ...
\end{verbatim}
This approach also enables the extent of the arrays ({\tt nx} and {\tt
  ny} in this example) to be specified explicitly.

The continuity kernel includes array accesses of the form {\tt
  a(i,j-1)}. If array {\tt a} is allocated with extent {\tt (Nx, Ny)}
then this specifies a location in memory {\tt Nx} words before that
occupied by {\tt a(i,j)}. In order for such an access to be aligned,
{\tt Nx} must be a multiple of the SIMD vector width (2 for SSE, 4 for
AVX) and this must be specified to the compiler. We therefore padded {\tt Nx}
as required and added a directive within the wrapper routine:
\begin{verbatim}
!DIR$ ASSUME (MOD(Nx,ALIGNMENT) .EQ. 0)
\end{verbatim}
where {\tt ALIGNMENT} is an integer parameter declared to have the value two
when compiling with SSE and four with AVX.

We can see in Figure~\ref{FIG_cont_vec} that these changes (``SSE
align'') significantly improve the performance of the smaller
domains. Again, as the domain size increases this improvement reduces
until, for $512^2$ and $1024^2$ (where we are limited by the bandwidth
to main memory), this optimisation has no effect.

In what follows, the code modifications were no longer restricted to
those that maintained correctness since we were interested in what
characteristics of the code were limiting its performance.

Since the Continuity kernel is one of several in the time-stepping
loop, we experimented with pre-loading the fields into cache simply by
executing the Continuity kernel immediately prior to executing it for
a second time with timing. Figure~\ref{FIG_cont_vec} shows that this
``pre-loading'' slightly improved the kernel performance for those
problem sizes that fit within cache. {\bf Discuss this}

Once all of the above steps were complete, the only un-aligned array
accesses reported by the compiler were those of the form {\tt
  a(i-1,j)}. Without taking a physical copy of the array {\tt a}, such
unaligned accesses are unavoidable in a stencil-based
finite-difference code. However, in order to quantify how much these
accesses limit the performance of the kernel we experimented with
replacing e.g. {\tt a(i-1,j)} with {\tt a(i-ALIGNMENT,j)}. The
resulting kernel had no un-aligned accesses and performed
significantly better; actually reaching or exceeding the L3-bandwidth
roofline for domains of $52^2$ -- $256^2$ (Figure~\ref{FIG_cont_vec} -
``align all'').

The final SIMD-related optimisations are concerned with what happens
at the beginning and end of a SIMD-vectorisable loop. If the loop
updates {\it e.g.}\ {\tt a(i,j)} and begins on a value of {\tt i} that
results in unaligned memory accesses ({\it e.g.}\ {\tt i = 2}) then
the compiler must insert a ``peel'' loop to handle those iterations
required to reach a point where memory is accessed on a 64-byte
boundary.  Similarly, if the trip-count of the resulting vectorisable
loop is not an integer multiple of the SIMD vector width then the
compiler must insert a ``remainder'' loop to handle those iterations
left over.

We removed the remainder loop by ensuring that the length of the inner
loop was a multiple of the vector width and adding an {\tt ASSUME}
compiler directive ({\it c.f.}\ that for {\tt Nx} above). This was
straightforward since we had already padded the array extents
themselves and NEMOLite2D uses a mask to define the active
computational domain - the additional grid points resulting from the
padding were simply marked as external to the domain.  Despite doing
this we found that the remainder loop was not removed until we
manually introduced the peel loop (rather than relying on the
compiler). Figure~\ref{FIG_cont_vec} shows that the resulting binary
was the most performant of all those compiled with SSE on all problem
sizes that are contained within L3 cache with the exception of $52^2$.
{\bf Why is $52^2$ an exception? Something to do with tuning it to fit in L2?}

\begin{verbatim}
!DIR$ VECTOR ALIGNED
do ic = 1, nrepeat, 1
  do jj = 2, N, 1

    ! Explicit peel loop
    do ji = 2, ALIGNMENT
       rtmp1 = (sshn_u(ji  ,jj ) + hu(ji  ,jj  ))*un(ji  ,jj)
       rtmp2 = (sshn_u(ji-1,jj ) + hu(ji-1,jj  ))*un(ji-1,jj)
       rtmp3 = (sshn_v(ji ,jj )  + hv(ji  ,jj  ))*vn(ji ,jj)
       rtmp4 = (sshn_v(ji ,jj-1) + hv(ji  ,jj-1))*vn(ji,jj-1)
       ssha(ji,jj) = sshn_t(ji,jj) + (rtmp2 - rtmp1 + rtmp4 - rtmp3) * &
             rdt / area_t(ji,jj)
    end do
    
    do ji = ALIGNMENT+1, M, 1

       rtmp1 = (sshn_u(ji  ,jj ) + hu(ji  ,jj  ))*un(ji  ,jj)
       rtmp2 = (sshn_u(ji-1,jj ) + hu(ji-1,jj))*un(ji-1,jj)
       rtmp3 = (sshn_v(ji ,jj )  + hv(ji  ,jj  ))*vn(ji ,jj)
       rtmp4 = (sshn_v(ji ,jj-1) + hv(ji ,jj-1))*vn(ji,jj-1)

       ssha(ji,jj) = sshn_t(ji,jj) + (rtmp2 - rtmp1 + rtmp4 - rtmp3) * &
                     rdt / area_t(ji,jj)
    end do
    
  end do
end do
\end{verbatim}

Since the caches of both of the CPUs used are eight-way associative,
we also considered whether we might be experiencing ``cache
thrashing'' whereby different locations in memory are mapped to the
same location in cache. We therefore constructed a version of the
kernel with the number of distinct streams reduced from twelve to
eight. In order to maintain the same Arithmetic Intensity as the
original kernel we also reduced the number of FLOPs. The inner loop of
the resulting kernel was:
\begin{verbatim}
do ji = ALIGNMENT+1, M, 1
   rtmp1 = (sshn_u(ji  ,jj ) + hu(ji  ,jj  ))*un(ji  ,jj)
   rtmp2 = (sshn_u(ji-ALIGNMENT,jj ) + hu(ji-ALIGNMENT,jj))* &
           un(ji-ALIGNMENT,jj)
   rtmp3 = (sshn_v(ji ,jj )  + hv(ji  ,jj  ))*vn(ji ,jj)
   ssha(ji,jj) =  (rtmp2 - rtmp1 - rtmp3) / sshn_t(ji,jj)
end do
\end{verbatim}

Figure~\ref{FIG_cont_vec} shows that the absolute performance of this
modified kernel (``8 arrays'') is mostly lower than that of the kernel
with twelve streams and certainly never exceeds it. We therefore
conclude that cache-thrashing is not a problem for this kernel.

Figure~\ref{FIG_kernel_perf} shows the performance of the Continuity
and u-Momentum kernels when compiled without vectorisation
(``-no-vec''), with SSE vectorisation (``-axSSE4.2'') and with AVX
(``-xHost''). SSE supports vector lengths of 128 bits (two
double-precision words) and AVX 512 bits (four double-precision
words). For the 64, 128 and 256 problem sizes, changing from no
vectorisation to SSE increases the performance of the Continuity
kernel by up to roughly 60\%; some way short of the ideal speed-up of
100\% that would be obtained with perfect SSE vectorisation of a
compute-bound kernel. Unsurprisingly then, moving from SSE to AVX
vectors does not give any performance improvement and in fact, is
slightly detrimental for most of the problem sizes.

\begin{figure}
  \centering
  \includegraphics[width=120mm]{nemolite2d_kernel_perf}
  \caption{Performance of the Continuity and u-Momentum kernels as a
    function of SIMD vectorisation and problem size. Results are for
    v.16 of the Intel compiler on a Xeon E5-1620 v.2 CPU.{\bf This plot
requires updating or dropping in favour of the clustered-bar form.}}
  \label{FIG_kernel_perf}
\end{figure}

Peak absolute performance is obtained for the $256^2$ problem size
with SSE instructions. The 512 problem size spills from L3 cache and
becomes bound by the bandwidth to main memory: SIMD vectorisation
gives no performance benefit. The fact that best performance is
obtained when the problem size fits within L3 indicates that there are
other overheads that dominate when the problem size is small enough to
fit within L2. We checked the efficiency of the SIMD vectorised code
by looking at the compiler diagnostic output. On the basis of this we
added Intel-specific directives to the code in order to specify that
the arrays were aligned on 64-byte boundaries. We also manually coded
the 'peel loop' (iterations required to reach a 64-byte
boundary). Having done this the compiler reported that no peel or
remainder loops were generated and the resulting (SSE) vectorised loop
had an estimated speed-up of 1.9. (This estimated speed-up was
previously 1.6; in agreement with the speed-up we measured.) After
making these changes the performance of the Continuity kernel with the
$32^2$ domain increased from 4.22 to 4.57 GFLOPs.

Turning now to the u-Momentum kernel (lower plot in
Figure~\ref{FIG_kernel_perf}), it is striking that the kernel
performance is almost independent of problem size. The reason for this
becomes clear when we edit the kernel to remove the many IF statements
(present because of the application of boundary conditions). The blue
symbols mark the resulting performance which is significantly
increased and again reaches a maximum at the 256 problem size.  It
should be noted that the compiler reported that this loop had been
vectorised, even before the IF statements were removed. Clearly the
vectorisation is more efficient once the code is simplified. However,
even with the up-lift in performance given by removing all the IF
statements, the performance of this kernel is still far short of even
the main-memory bandwidth limit, despite the fact that the $256^2$
problem fits within L3 cache.

In order to help further understand the performance of the Continuity
kernel, in Figure~\ref{FIG_cont_perf} we plot the performance of the
Continuity kernel alongside HWPC-derived metrics (obtained using the
likwid tool). We use the version compiled with SSE instructions since
that gave the best performance. This plot confirms that the $32^2$
problem size is dominated by bandwidth to L2. Sizes $64^2$, $128^2$
and $256^2$ are dominated by bandwidth to L2 and L3 and $512^2$ makes
heavy use of main memory.

\begin{figure}
  \centering
  \includegraphics[width=120mm]{nemolite2d_cont_perf}
  \caption{Performance of the Continuity kernel (open circles -- right y-axis)
    and associated HWPC data (bars -- left y-axis) as a function of
    problem size. Results are for v.16 of the Intel compiler on a Xeon
    E5-1620 v.2 CPU using only the SSE SIMD instructions.}
  \label{FIG_cont_perf}
\end{figure}

\subsection{Tracer Advection in NEMO}

\section{Conclusions}

\begin{itemize}
  
\item Constructing a traditional roofline model is not that straightforward.
  
\item We have demonstrated that the upper bounds in such a model can be
  unrealistically high and therefore not that useful in determining
  the performance-limiting aspects of a kernel.

\item We have developed a tool, Habakkuk, which parses Fortran source
  code and generates various performance estimates using a simple
  model of the microarchitecture of the target CPU.

\item Using the resulting performance estimates as the ceilings in the
  roofline model results in more accurate bounds and provides more
  information to a developer seeking to optimise their source code.

\end{itemize}

\bibliographystyle{unsrt}
\bibliography{habakkuk}

\end{document}

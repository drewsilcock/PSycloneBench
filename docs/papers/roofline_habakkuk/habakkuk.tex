\documentclass[12pt]{article}
\usepackage[pdftex]{graphicx}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage{url}

\newcommand{\psykal}{{PS}y{KA}l}

\begin{document}

\title{Analysing the Performance of Shallow-Water Models with the
 Roofline Model}

\author{A.~.R.~Porter and R.~W.~Ford}

\maketitle

\section{Introduction}

Shallow implementation of Sadourny~\cite{sadourny75}.
NEMOLite2D.

\section{The Roofline Model and its Limitations}

\subsection{What the Roofline Model (RFM) is}

Arithmetic intensity and MAX(memory bandwidth, FLOPS)

\subsection{Approaches to constructing a RFM}

Theoretical limits versus experimentally-measured values (STREAM and
LINPACK).

Measuring Arithmetic Intensity. Hardware Performance Counters (access,
reliability, difficulty of interpretation) or eye-balling the code
(error prone and time consuming).

\subsection{Basic Assumptions}
\subsection{When the assumptions break down}

All FLOPs are not equal. For instance, on Ivy Bridge a division costs
approximately eight times as much as a multiplication.
Instruction-level parallelism - operations despatched to different
execution ports can be executed in parallel.

\section{How to remove some of the assumptions}

Parse the (Fortran) source code.  Construct a Directed Acyclic Graph
to represent the source.  Use the properties of the DAG to
characterise the processed source.  At its simplest, this can just be
a count of FLOPs and memory (array) accesses.  Can then be extended to
allow for caching of consecutive array accesses as in stencil
operations in a direct-addressed model.

\subsubsection{Scheduling}

Obtaining a performance estimate by simply counting all of the FLOPs
that must be performed is equivalent to assuming that they are all
executed sequentially. However, a core on any recent generation of CPU
is likely to have support for Instruction Level Parallelism.

\subsection{Single Instruction Multiple Data}

\subsection{Multi-core Performance}

So far we have considered only single-core performance. In reality,
any application seeking to achieve good performance will run in
parallel on multiple CPU cores.  If we restrict ourselves to running a
single process/thread per core then clearly that thread will have
exclusive use of that core's registers and execution units. Resources
shared between cores (such as L2 cache or bandwidth to main memory) will
however be shared by the executing threads.

\section{Results}

\subsection{Shallow}

\subsection{NEMOLite2D}

\subsection{Tracer Advection in NEMO}

\section{Conclusions}

\bibliographystyle{unsrt}
\bibliography{habakkuk}

\end{document}
